"""
æ–‡ç« ç¿»è¯‘å¼•æ“ï¼ˆç²¾ç®€ç‰ˆï¼‰
åªä¿ç•™æ ¸å¿ƒç¿»è¯‘åŠŸèƒ½ï¼Œä½†ä¿ç•™å®Œæ•´çš„æœ¯è¯­åº“é€»è¾‘
"""

import re
import requests
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry as URLLibRetry
from retry_utils import APIRetryHandler, RetryConfig


class RateLimiter:
    """è‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self, initial_workers: int, max_workers: int, min_workers: int,
                 backoff: float, increase: float, success_threshold: float, increase_interval: int):
        self.current_workers = initial_workers
        self.max_workers = max_workers
        self.min_workers = min_workers
        self.backoff = backoff
        self.increase = increase
        self.success_threshold = success_threshold
        self.increase_interval = increase_interval

        self.success_count = 0
        self.total_count = 0
        self.last_increase_time = time.time()
        self.lock = Lock()

    def on_rate_limit_error(self):
        """é‡åˆ°429é”™è¯¯ï¼Œé™ä½å¹¶å‘"""
        with self.lock:
            old_workers = self.current_workers
            self.current_workers = max(self.min_workers, int(self.current_workers * self.backoff))
            print(f"âš ï¸ é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œé™ä½å¹¶å‘: {old_workers} -> {self.current_workers}")

    def on_success(self):
        """æˆåŠŸè¯·æ±‚ï¼Œç»Ÿè®¡æˆåŠŸç‡"""
        with self.lock:
            self.success_count += 1
            self.total_count += 1

            # è®¡ç®—æˆåŠŸç‡
            if self.total_count >= 20:  # è‡³å°‘20ä¸ªæ ·æœ¬
                success_rate = self.success_count / self.total_count
                current_time = time.time()

                # å¦‚æœæˆåŠŸç‡é«˜ä¸”è·ç¦»ä¸Šæ¬¡å¢åŠ å·²è¿‡ä¸€æ®µæ—¶é—´
                if (success_rate >= self.success_threshold and
                        current_time - self.last_increase_time >= self.increase_interval and
                        self.current_workers < self.max_workers):
                    old_workers = self.current_workers
                    self.current_workers = min(self.max_workers, int(self.current_workers * self.increase))
                    self.last_increase_time = current_time
                    print(f"âœ“ æå‡å¹¶å‘: {old_workers} -> {self.current_workers}")

                    # é‡ç½®è®¡æ•°å™¨
                    self.success_count = 0
                    self.total_count = 0

    def on_failure(self):
        """è¯·æ±‚å¤±è´¥ï¼ˆé429é”™è¯¯ï¼‰"""
        with self.lock:
            self.total_count += 1

    def get_current_workers(self) -> int:
        """è·å–å½“å‰å¹¶å‘æ•°"""
        with self.lock:
            return self.current_workers


class ArticleTranslator:
    """æ–‡ç« ç¿»è¯‘å¼•æ“"""

    def __init__(
        self,
        api_key: str,
        api_url: str,
        model: str,
        glossary: Optional[Dict[str, str]] = None,
        case_sensitive: bool = False,
        whole_word_only: bool = True,
        config: Optional[Dict] = None
    ):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨

        Args:
            api_key: APIå¯†é’¥
            api_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            glossary: æœ¯è¯­è¡¨å­—å…¸ {"English": "ä¸­æ–‡"}
            case_sensitive: æœ¯è¯­æ›¿æ¢æ˜¯å¦åŒºåˆ†å¤§å°å†™ï¼ˆé»˜è®¤Falseï¼‰
            whole_word_only: æ˜¯å¦åªåŒ¹é…å®Œæ•´å•è¯ï¼ˆé»˜è®¤Trueï¼‰
            config: é…ç½®å­—å…¸ï¼ˆç”¨äºè¯»å–APIå‚æ•°å’Œå¹¶å‘é…ç½®ï¼‰
        """
        self.api_key = api_key
        self.api_url = api_url.rstrip('/')
        self.model = model
        self.chat_endpoint = f"{self.api_url}/chat/completions"
        self.glossary = glossary or {}
        self.case_sensitive = case_sensitive
        self.whole_word_only = whole_word_only

        # ä»configè¯»å–å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        self.config = config or {}
        self.timeout = self.config.get('api', {}).get('timeout', 120)
        self.temperature = self.config.get('api', {}).get('temperature', 0.3)
        self.max_tokens = self.config.get('api', {}).get('max_tokens', 65536)

        # ===== æ–°å¢ï¼šåˆ›å»ºå…±äº«çš„ Session å¯¹è±¡è¿›è¡Œè¿æ¥å¤ç”¨ =====
        self.session = requests.Session()

        # é…ç½®è¿æ¥æ± ï¼šæ± å¤§å° = æœ€å¤§å¹¶å‘æ•° * 2
        max_workers = self.config.get('concurrency', {}).get('max_translation_workers', 100)
        pool_size = min(max_workers * 2, 200)  # é™åˆ¶æœ€å¤§200

        # é…ç½® HTTPAdapterï¼ˆè¿æ¥å¤ç”¨å’Œè¿æ¥æ± ç®¡ç†ï¼‰
        adapter = HTTPAdapter(
            pool_connections=pool_size,      # è¿æ¥æ± æ•°é‡
            pool_maxsize=pool_size,          # è¿æ¥æ± æœ€å¤§å¤§å°
            max_retries=0,                   # ç¦ç”¨urllib3è‡ªåŠ¨é‡è¯•ï¼ˆæˆ‘ä»¬ç”¨è‡ªå·±çš„é‡è¯•é€»è¾‘ï¼‰
            pool_block=True                  # è¿æ¥æ± æ»¡æ—¶é˜»å¡ç­‰å¾…ï¼ˆé¿å…åˆ›å»ºè¿‡å¤šè¿æ¥ï¼‰
        )

        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        self.session.headers.update({
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Connection": "keep-alive"       # ä¿æŒè¿æ¥
        })

        # å¼ºåˆ¶ç¦ç”¨ä»£ç†ï¼ˆå¤šç§æ–¹å¼ç¡®ä¿ç”Ÿæ•ˆï¼‰
        self.session.proxies = {}
        self.session.trust_env = False  # å¿½ç•¥ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®
        # ===== è¿æ¥å¤ç”¨é…ç½®ç»“æŸ =====

        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        concurrency_config = self.config.get('concurrency', {})
        self.rate_limiter = RateLimiter(
            initial_workers=concurrency_config.get('initial_translation_workers', 20),
            max_workers=concurrency_config.get('max_translation_workers', 100),
            min_workers=concurrency_config.get('min_translation_workers', 1),
            backoff=concurrency_config.get('rate_limit_backoff', 0.5),
            increase=concurrency_config.get('rate_limit_increase', 1.2),
            success_threshold=concurrency_config.get('success_threshold', 0.95),
            increase_interval=concurrency_config.get('increase_interval', 30)
        )

        # åˆå§‹åŒ–é‡è¯•å¤„ç†å™¨
        retry_config = self.config.get('retry', {})
        self.retry_handler = APIRetryHandler(
            config=RetryConfig(
                max_retries=retry_config.get('translation_max_retries', 3),
                initial_delay=retry_config.get('translation_initial_delay', 1.0),
                max_delay=retry_config.get('translation_max_delay', 30.0),
                exponential_base=retry_config.get('translation_exponential_base', 2.0),
                retry_on_dns_error=retry_config.get('retry_on_dns_error', True),
                retry_on_connection_error=retry_config.get('retry_on_connection_error', True),
                retry_on_timeout=retry_config.get('retry_on_timeout', True),
                retry_on_5xx=retry_config.get('retry_on_5xx', True),
                retry_on_429=retry_config.get('retry_on_429_translation', False)  # 429ç”±rate_limiterå¤„ç†
            ),
            logger=None,  # ç¿»è¯‘å™¨é€šå¸¸æ²¡æœ‰loggerï¼Œä½¿ç”¨print
            context_provider=lambda: f"[æ–‡ä»¶: {self.current_file}]"  # æä¾›æ–‡ä»¶ä¸Šä¸‹æ–‡
        )

        # æœ¯è¯­æ›¿æ¢ç»Ÿè®¡
        self.total_replacements = 0
        self.total_terms_used = 0
        self._replacement_lock = Lock()

        # æ—¥å¿—ç›¸å…³ï¼ˆæ¯ä¸ªå­è¿›ç¨‹æœ‰ç‹¬ç«‹å®ä¾‹ï¼Œä¸éœ€è¦é”ï¼‰
        self.log_dir = Path("logs/translation")
        self.current_file = "unknown"
        self.request_counter = 0

        # å¤±è´¥æ–‡æœ¬è®°å½•
        self.failed_texts_log = Path("logs/total_issue_files.jsonl")
        self.failed_texts_log.parent.mkdir(parents=True, exist_ok=True)

        # å¤‡ç”¨æ¨¡å‹é…ç½®ï¼ˆç”¨äºè´¨é‡é—®é¢˜æ—¶åˆ‡æ¢ï¼‰- ä»configè¯»å–
        self.fallback_model = self.config.get('api', {}).get('fallback_translation_model', 'gemini-2.0-flash-exp')
        self.original_model = self.model  # ä¿å­˜åŸå§‹æ¨¡å‹

    def translate(self, text: str, context: Optional[Dict] = None, text_id: Optional[str] = None) -> str:
        """
        ç¿»è¯‘æ–‡æœ¬

        Args:
            text: å¾…ç¿»è¯‘æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ {
                'chapter_title': 'ç« èŠ‚æ ‡é¢˜',
                'chapter_summary': 'ç« èŠ‚æ‘˜è¦',
                'keywords': ['å…³é”®è¯1', 'å…³é”®è¯2']
            }
            text_id: æ–‡æœ¬å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå¤±è´¥è¿½è¸ªï¼‰

        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        if not text or not text.strip():
            return ""

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆé˜²æ­¢è¶…é•¿è¯·æ±‚ï¼‰
        text_length = len(text)
        if text_length > 50000:  # è¶…è¿‡5ä¸‡å­—ç¬¦
            print(f"[WARNING] Text too long: {text_length} chars, will split")
            # åˆ†æ®µç¿»è¯‘
            return self._translate_long_text(text, context)

        # æ­£å¸¸ç¿»è¯‘æµç¨‹

        # 1. åº”ç”¨æœ¯è¯­è¡¨ï¼ˆä¸æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼‰
        text_with_glossary, replacement_count = self.apply_glossary(text, show_log=False)

        # ç´¯è®¡æœ¯è¯­æ›¿æ¢ç»Ÿè®¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        if replacement_count > 0:
            with self._replacement_lock:
                self.total_replacements += replacement_count

        # 2. æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(text_with_glossary, context)

        # è·å–è¯·æ±‚IDï¼ˆå­è¿›ç¨‹ç‹¬ç«‹ï¼Œä¸éœ€è¦é”ï¼‰
        self.request_counter += 1
        request_id = self.request_counter

        # 3. è°ƒç”¨APIï¼ˆå¸¦è´¨é‡æ£€æŸ¥çš„é‡è¯•æœºåˆ¶ï¼‰
        start_time = time.time()

        payload = None
        response_json = None
        final_error = None

        # ä»é…ç½®è¯»å–æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬è´¨é‡æ£€æŸ¥å¤±è´¥çš„é‡è¯•ï¼‰
        max_quality_retries = self.config.get('retry', {}).get('translation_max_retries', 30)

        # è·Ÿè¸ªè¿ç»­"å®Œå…¨æœªç¿»è¯‘"çš„æ¬¡æ•°
        consecutive_untranslated = 0
        max_consecutive_untranslated = 3  # è¿ç»­3æ¬¡å®Œå…¨æœªç¿»è¯‘å°±æ”¾å¼ƒ

        # æ ‡è®°æ˜¯å¦å·²ç»åˆ‡æ¢åˆ°fallbackæ¨¡å‹
        switched_to_fallback = False

        for attempt in range(max_quality_retries):
            attempt_start = time.time()
            try:
                # æ·»åŠ å°å»¶è¿Ÿï¼ˆå‡è½»æœåŠ¡å™¨å‹åŠ›ï¼Œé¿å…è¿æ¥è¢«å¼ºåˆ¶å…³é—­ï¼‰
                if attempt > 0:
                    time.sleep(0.1 * attempt)

                # å¦‚æœç¬¬ä¸€æ¬¡å°è¯•å¤±è´¥ä¸”è¿˜æœªåˆ‡æ¢ï¼Œåˆ™ä¸´æ—¶åˆ‡æ¢åˆ°fallbackæ¨¡å‹
                if attempt == 1 and not switched_to_fallback and self.fallback_model:
                    print(f"  â†’ åˆ‡æ¢åˆ°æ›´å¥½çš„æ¨¡å‹: {self.fallback_model}")
                    self.model = self.fallback_model
                    switched_to_fallback = True

                payload, response_json, translation = self._call_llm(prompt, request_id)

                # æ¸…ç†ç¿»è¯‘ç»“æœ
                translation = self._clean_output(translation)

                # ===== æ–°å¢ï¼šç¿»è¯‘è´¨é‡æ£€æŸ¥ =====
                quality_check_passed, issue_reason = self._check_translation_quality(
                    original_text=text,
                    translated_text=translation
                )

                if not quality_check_passed:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯"å®Œå…¨æœªç¿»è¯‘"
                    is_untranslated = "å®Œå…¨æœªç¿»è¯‘" in issue_reason

                    if is_untranslated:
                        consecutive_untranslated += 1
                    else:
                        consecutive_untranslated = 0  # é‡ç½®è®¡æ•°

                    # è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œè®°å½•å¹¶é‡è¯•
                    print(f"[WARNING] [æ–‡ä»¶: {self.current_file}] ç¿»è¯‘è´¨é‡å¼‚å¸¸: {issue_reason}")
                    print(f"  åŸæ–‡é•¿åº¦: {len(text)}, è¯‘æ–‡é•¿åº¦: {len(translation)}")

                    # å¦‚æœè¿ç»­å¤šæ¬¡å®Œå…¨æœªç¿»è¯‘ï¼Œæå‰æ”¾å¼ƒ
                    if consecutive_untranslated >= max_consecutive_untranslated:
                        print(f"  âœ— è¿ç»­{consecutive_untranslated}æ¬¡å®Œå…¨æœªç¿»è¯‘ï¼Œå¯èƒ½æ˜¯OCRé”™è¯¯æˆ–APIæ— æ³•è¯†åˆ«çš„æ–‡æœ¬")
                        print(f"  â†’ åœæ­¢é‡è¯•ï¼Œè¿”å›åŸæ–‡")
                        self._log_quality_issue(
                            request_id=request_id,
                            original_text=text,
                            translated_text=translation,
                            issue_reason=f"{issue_reason} (è¿ç»­{consecutive_untranslated}æ¬¡ï¼Œåœæ­¢é‡è¯•)",
                            attempt=attempt + 1,
                            used_fallback_model=switched_to_fallback
                        )
                        # æ¢å¤åŸå§‹æ¨¡å‹
                        if switched_to_fallback:
                            self.model = self.original_model
                        return text

                    if attempt < max_quality_retries - 1:
                        print(f"  â†’ æ­£åœ¨é‡æ–°ç¿»è¯‘ (ç¬¬{attempt + 2}æ¬¡å°è¯•)...")
                        # è®°å½•è´¨é‡é—®é¢˜
                        self._log_quality_issue(
                            request_id=request_id,
                            original_text=text,
                            translated_text=translation,
                            issue_reason=issue_reason,
                            attempt=attempt + 1,
                            used_fallback_model=switched_to_fallback
                        )
                        continue  # é‡æ–°å°è¯•
                    else:
                        # å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›åŸæ–‡
                        print(f"  âœ— å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°({max_quality_retries})ï¼Œè¿”å›åŸæ–‡")
                        self._log_quality_issue(
                            request_id=request_id,
                            original_text=text,
                            translated_text=translation,
                            issue_reason=f"{issue_reason} (å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›åŸæ–‡)",
                            attempt=attempt + 1,
                            used_fallback_model=switched_to_fallback
                        )
                        # æ¢å¤åŸå§‹æ¨¡å‹
                        if switched_to_fallback:
                            self.model = self.original_model
                        # ç›´æ¥è¿”å›åŸæ–‡ï¼Œä¸ä½¿ç”¨æœ‰é—®é¢˜çš„è¯‘æ–‡
                        return text

                # è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œé‡ç½®è®¡æ•°å™¨
                consecutive_untranslated = 0

                # è®°å½•æˆåŠŸçš„è¯·æ±‚å’Œå“åº”
                self._log_translation(
                    request_id=request_id,
                    payload=payload,
                    response=response_json,
                    error=None,
                    attempts=attempt + 1
                )

                # æ¢å¤åŸå§‹æ¨¡å‹ï¼ˆæˆåŠŸç¿»è¯‘åï¼‰
                if switched_to_fallback:
                    self.model = self.original_model

                return translation

            except Exception as e:
                final_error = str(e)

                # æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ˆæ›´è¯¦ç»†ï¼‰
                error_preview = final_error[:200] if len(final_error) > 200 else final_error
                print(f"[WARNING] [æ–‡ä»¶: {self.current_file}] ç¿»è¯‘è¯·æ±‚å¤±è´¥ (ç¬¬{attempt + 1}/{max_quality_retries}æ¬¡): {error_preview}")

                if attempt < max_quality_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"  â†’ {wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    # æœ€åä¸€æ¬¡å¤±è´¥ï¼Œè®°å½•é”™è¯¯
                    print(f"  âœ— å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°({max_quality_retries})ï¼Œè¿”å›åŸæ–‡")

                    # è®°å½•å¤±è´¥æ–‡æœ¬
                    self._log_failed_text(
                        text_id=text_id,
                        original_text=text,
                        error=final_error,
                        attempts=max_quality_retries,
                        context=context
                    )

                    self._log_translation(
                        request_id=request_id,
                        payload=payload,
                        response=None,
                        error=final_error,
                        attempts=attempt + 1
                    )
                    # æ¢å¤åŸå§‹æ¨¡å‹ï¼ˆå¤±è´¥æ—¶ï¼‰
                    if switched_to_fallback:
                        self.model = self.original_model
                    # è¿”å›åŸæ–‡ï¼ˆä¸ä¼šå½±å“æ•´ä¸ªæ–‡ä»¶ï¼‰
                    return text

    def _call_llm(self, prompt: str, request_id: int) -> str:
        """
        è°ƒç”¨LLM APIï¼ˆä½¿ç”¨ Session è¿›è¡Œè¿æ¥å¤ç”¨ï¼‰

        Args:
            prompt: æç¤ºè¯
            request_id: è¯·æ±‚IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰

        Returns:
            (payload, response_json, translated_text) å…ƒç»„
        """
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯æ–‡æ¡£ç¿»è¯‘åŠ©æ‰‹ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": False
        }

        # ç”¨äºæ”¶é›†é‡è¯•äº‹ä»¶
        retry_events = []
        result = None
        final_error = None

        # é‡è¯•å›è°ƒå‡½æ•°
        def on_retry(attempt: int, error_type: str, error_detail: str):
            retry_events.append({
                "attempt": attempt,
                "error_type": error_type,
                "error_detail": error_detail,
                "timestamp": datetime.now().isoformat()
            })

        # ä½¿ç”¨é‡è¯•å¤„ç†å™¨åŒ…è£…APIè°ƒç”¨
        def _make_api_call():
            # ä½¿ç”¨å…±äº«çš„ Session å¯¹è±¡ï¼ˆè‡ªåŠ¨å¤ç”¨è¿æ¥ï¼‰
            response = self.session.post(
                self.chat_endpoint,
                json=payload,
                timeout=self.timeout,
                verify=True
            )

            # å¤„ç†429é”™è¯¯
            if response.status_code == 429:
                self.rate_limiter.on_rate_limit_error()
                response.raise_for_status()

            response.raise_for_status()

            # å°è¯•è§£æJSONï¼Œå¤±è´¥æ—¶æ˜¾ç¤ºåŸå§‹å“åº”
            try:
                return response.json()
            except json.JSONDecodeError as e:
                # JSONè§£æå¤±è´¥ï¼Œè®°å½•åŸå§‹å“åº”
                raw_text = response.text[:1000]  # åªå–å‰1000å­—ç¬¦
                error_msg = f"JSONè§£æå¤±è´¥: {str(e)}\nåŸå§‹å“åº”: {raw_text}"
                raise Exception(error_msg)

        try:
            # æ‰§è¡Œå¸¦é‡è¯•çš„APIè°ƒç”¨
            result = self.retry_handler.execute_with_retry(_make_api_call, on_retry_callback=on_retry)

            # è®°å½•æˆåŠŸ
            self.rate_limiter.on_success()

            # å®‰å…¨åœ°æå–ç¿»è¯‘æ–‡æœ¬ï¼Œå¤„ç†å¯èƒ½çš„ç»“æ„é”™è¯¯
            try:
                translated_text = result['choices'][0]['message']['content'].strip()
            except (KeyError, IndexError, TypeError) as e:
                # APIè¿”å›ç»“æ„ä¸ç¬¦åˆé¢„æœŸ
                error_msg = f"APIè¿”å›ç»“æ„é”™è¯¯: {str(e)}\nè¿”å›å†…å®¹: {str(result)[:500]}"
                raise Exception(error_msg)

            return payload, result, translated_text

        except Exception as e:
            # è®°å½•å¤±è´¥ä¿¡æ¯
            final_error = str(e)
            raise

        finally:
            # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è®°å½•é‡è¯•äº‹ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
            if retry_events:
                self._log_retry_events(
                    request_id=request_id,
                    payload=payload,
                    response=result,
                    retry_events=retry_events,
                    final_error=final_error
                )

    def apply_glossary(self, text: str, show_log: bool = False) -> Tuple[str, int]:
        """
        åº”ç”¨æœ¯è¯­åº“è¿›è¡Œé¢„ç¿»è¯‘æ›¿æ¢ï¼ˆå®Œæ•´ç‰ˆé€»è¾‘ï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬
            show_log: æ˜¯å¦æ˜¾ç¤ºæ›¿æ¢æ—¥å¿—

        Returns:
            (æ›¿æ¢åçš„æ–‡æœ¬, æ›¿æ¢æ¬¡æ•°)
        """
        if not self.glossary or not text:
            return text, 0

        # URLä¿æŠ¤
        modified_text, url_placeholders = self._protect_urls(text)

        # æœ¯è¯­æ›¿æ¢
        replacement_count = 0
        replaced_terms = []

        # æŒ‰æœ¯è¯­é•¿åº¦æ’åºï¼ˆé•¿çš„å…ˆæ›¿æ¢ï¼‰
        sorted_terms = sorted(self.glossary.items(), key=lambda x: len(x[0]), reverse=True)

        for source_term, target_term in sorted_terms:
            if not source_term or not target_term:
                continue

            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼
            pattern = r'\b' + re.escape(source_term) + r'\b' if self.whole_word_only else re.escape(source_term)
            flags = 0 if self.case_sensitive else re.IGNORECASE

            # æŸ¥æ‰¾åŒ¹é…
            matches = re.findall(pattern, modified_text, flags=flags)
            if matches:
                count = len(matches)
                modified_text = re.sub(pattern, target_term, modified_text, flags=flags)
                replacement_count += count
                replaced_terms.append((source_term, target_term, count))

        # æ˜¾ç¤ºæ›¿æ¢æ—¥å¿—
        if show_log and replaced_terms:
            print(f"  æœ¯è¯­æ›¿æ¢: {len(replaced_terms)} ä¸ªæœ¯è¯­ï¼Œå…± {replacement_count} å¤„")

        # æ¢å¤URL
        modified_text = self._restore_urls(modified_text, url_placeholders)

        return modified_text, replacement_count

    def _protect_urls(self, text: str) -> Tuple[str, Dict[str, str]]:
        """
        æå–URLå¹¶ç”¨å ä½ç¬¦æ›¿æ¢

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            (æ›¿æ¢åçš„æ–‡æœ¬, {å ä½ç¬¦: URL})
        """
        # åˆå¹¶URLåŒ¹é…æ­£åˆ™
        url_pattern = (
            r'(?:https?|ftp|ftps)://[^\s<>"\'\)]+|'
            r'(?:dx\.)?doi\.org/[^\s<>"\'\)]+|'
            r'www\.[a-zA-Z0-9][-a-zA-Z0-9]*\.[^\s<>"\'\)]+|'
            r'\[([^\]]+)\]\(([^\)]+)\)'
        )

        urls = re.findall(url_pattern, text)

        # å±•å¹³Markdowné“¾æ¥
        url_list = []
        for match in urls:
            if isinstance(match, tuple):
                url_list.append(f'[{match[0]}]({match[1]})')
            else:
                url_list.append(match)

        # å»é‡å¹¶æŒ‰é•¿åº¦æ’åº
        url_list = sorted(set(url_list), key=len, reverse=True)

        # åˆ›å»ºå ä½ç¬¦
        url_placeholders = {}
        modified_text = text
        for i, url in enumerate(url_list):
            placeholder = f"__URL_PLACEHOLDER_{i}__"
            url_placeholders[placeholder] = url
            modified_text = modified_text.replace(url, placeholder)

        return modified_text, url_placeholders

    def _restore_urls(self, text: str, url_placeholders: Dict[str, str]) -> str:
        """æ¢å¤URLå ä½ç¬¦"""
        for placeholder, url in url_placeholders.items():
            text = text.replace(placeholder, url)
        return text

    def _build_prompt(self, text: str, context: Optional[Dict]) -> str:
        """
        æ„å»ºç¿»è¯‘æç¤ºè¯

        Args:
            text: å¾…ç¿»è¯‘æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            å®Œæ•´æç¤ºè¯
        """
        prompt_parts = [
            "è¯·å°†ä»¥ä¸‹è‹±è¯­æˆ–è€…ä¿„è¯­ç¿»è¯‘æˆä¸­æ–‡",
            "",
            "è¦æ±‚ï¼š",
            "1. ä¿æŒå­¦æœ¯é£æ ¼å’Œä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§",
            "2. ä¿ç•™åŸæ–‡çš„æ®µè½ç»“æ„å’Œæ ¼å¼",
            "3. **ä¿æŒæ‰€æœ‰URLé“¾æ¥ï¼ˆhttp://æˆ–https://å¼€å¤´ï¼‰åŸæ ·ä¸å˜ï¼Œä¸è¦ç¿»è¯‘æˆ–ä¿®æ”¹**",
            "4. **ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœ**ï¼Œä¸¥ç¦åºŸè¯ã€ä¸¥ç¦åˆ†æ",
            "5. ä¸è¦æ·»åŠ \"è¯‘æ–‡:\"ã€\"ç¿»è¯‘:\"ç­‰å‰ç¼€",
            "6. å¦‚æœæœ‰è¢«è¯¯ç¿»è¯‘ã€è¯¯æœ¯è¯­æ›¿æ¢çš„URLï¼Œè®°å¾—è¿›è¡Œä¿®å¤",
            "7. å‘é€ç»™ä½ çš„æ‰€æœ‰æ–‡æœ¬éƒ½éœ€è¦è¢«ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œä¸è¦æ¼è¯‘",
            "8. **å¦‚æœé‡åˆ°OCRè¯†åˆ«é”™è¯¯æˆ–æ— æ³•è¯†åˆ«çš„æ··ä¹±æ–‡æœ¬ï¼Œè¯·å°½åŠ›ç¿»è¯‘å¯è¯†åˆ«éƒ¨åˆ†ï¼Œæ— æ³•è¯†åˆ«çš„ä¿æŒåŸæ ·**"
        ]

        # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ˜ç¡®çš„åˆ†éš”ç¬¦ï¼Œé¿å…è¢«ç¿»è¯‘ï¼‰
        if context:
            prompt_parts.append("")
            prompt_parts.append("=" * 50)
            prompt_parts.append("ã€å‚è€ƒä¸Šä¸‹æ–‡ - ä¸è¦ç¿»è¯‘æ­¤éƒ¨åˆ†ã€‘")

            if context.get('chapter_title'):
                prompt_parts.append(f"ç« èŠ‚: {context['chapter_title']}")

            if context.get('chapter_summary'):
                prompt_parts.append(f"æ‘˜è¦: {context['chapter_summary']}")

            if context.get('keywords'):
                keywords = ', '.join(context['keywords'])
                prompt_parts.append(f"å…³é”®è¯: {keywords}")

            # æ·»åŠ ä¸Šä¸‹æ–‡çª—å£ï¼ˆå‰åæ–‡ï¼‰
            if context.get('prev_text') or context.get('next_text'):
                prompt_parts.append("")
                if context.get('prev_text'):
                    prev = context['prev_text'].strip()
                    if prev:
                        prompt_parts.append(f"ä¸Šæ–‡: ...{prev}")

                if context.get('next_text'):
                    next_text = context['next_text'].strip()
                    if next_text:
                        prompt_parts.append(f"ä¸‹æ–‡: {next_text}...")

            prompt_parts.append("=" * 50)

        # æ·»åŠ å¾…ç¿»è¯‘æ–‡æœ¬
        prompt_parts.append("")
        prompt_parts.append("ã€å¾…ç¿»è¯‘æ–‡æœ¬ã€‘")
        prompt_parts.append(text)
        prompt_parts.append("")
        prompt_parts.append("ã€è¯·ç›´æ¥è¾“å‡ºä¸­æ–‡ç¿»è¯‘ã€‘")

        return "\n".join(prompt_parts)

    def _clean_output(self, text: str) -> str:
        """
        æ¸…ç†ç¿»è¯‘ç»“æœä¸­çš„é¢å¤–æ ‡è®°

        Args:
            text: åŸå§‹ç¿»è¯‘ç»“æœ

        Returns:
            æ¸…ç†åçš„è¯‘æ–‡
        """
        cleaned = text.strip()

        # ç§»é™¤å¸¸è§çš„å‰ç¼€æ ‡è®°ï¼ˆåˆå¹¶æ­£åˆ™ï¼‰
        prefixes = r'^(?:è¯‘æ–‡|ç¿»è¯‘|ã€è¯‘æ–‡ã€‘|ã€ç¿»è¯‘ã€‘|\[è¯‘æ–‡\]|\[ç¿»è¯‘\]|Translation|ä»¥ä¸‹æ˜¯ç¿»è¯‘|ç¿»è¯‘å¦‚ä¸‹|ç¿»è¯‘ç»“æœ)[ï¼š:\s]+'
        cleaned = re.sub(prefixes, '', cleaned, flags=re.IGNORECASE)

        # ç§»é™¤é¦–å°¾çš„å¼•å·ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
        quote_pairs = [('"', '"'), ('ã€Œ', 'ã€'), ('ã€', 'ã€'), ('ã€Š', 'ã€‹')]
        for open_q, close_q in quote_pairs:
            if cleaned.startswith(open_q) and cleaned.endswith(close_q):
                cleaned = cleaned[1:-1]
                break

        return cleaned.strip()

    def translate_batch(self, tasks: List[Tuple[str, Optional[Dict]]]) -> List[str]:
        """
        æ‰¹é‡å¹¶å‘ç¿»è¯‘ï¼ˆä½¿ç”¨è‡ªé€‚åº”é€Ÿç‡é™åˆ¶ï¼‰

        Args:
            tasks: [(text, context), ...] å¾…ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨

        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        if not tasks:
            return []

        # é‡ç½®æœ¯è¯­æ›¿æ¢ç»Ÿè®¡
        self.total_replacements = 0

        results = [None] * len(tasks)

        # ä½¿ç”¨åŠ¨æ€å¹¶å‘æ•°
        def translate_single(index: int, text: str, context: Optional[Dict]) -> Tuple[int, str]:
            """ç¿»è¯‘å•ä¸ªæ–‡æœ¬å¹¶è¿”å›ç´¢å¼•å’Œç»“æœ"""
            # ä»contextä¸­æå–text_idï¼ˆå¦‚æœæœ‰ï¼‰
            text_id = context.get('text_id') if context else None
            translation = self.translate(text, context, text_id=text_id)
            return index, translation

        # å¹¶å‘ç¿»è¯‘
        with ThreadPoolExecutor(max_workers=self.rate_limiter.get_current_workers()) as executor:
            futures = {
                executor.submit(translate_single, i, text, context): i
                for i, (text, context) in enumerate(tasks)
            }

            for future in as_completed(futures):
                try:
                    index, translation = future.result()
                    results[index] = translation
                except Exception as e:
                    # å¤±è´¥æ—¶è¿”å›åŸæ–‡ï¼Œå¹¶æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                    index = futures[future]
                    results[index] = tasks[index][0]
                    self.rate_limiter.on_failure()

                    # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    error_msg = str(e)
                    if len(error_msg) > 200:
                        error_msg = error_msg[:200] + "..."
                    print(f"[ERROR] ç¿»è¯‘å¤±è´¥ (ä»»åŠ¡ {index+1}): {error_msg}")

        # æ˜¾ç¤ºæœ¯è¯­æ›¿æ¢æ€»è®¡
        if self.total_replacements > 0:
            print(f"\nğŸ“Š æœ¯è¯­æ›¿æ¢ç»Ÿè®¡: å…±æ›¿æ¢ {self.total_replacements} å¤„\n")

        return results

    def _translate_long_text(self, text: str, context: Optional[Dict] = None) -> str:
        """
        ç¿»è¯‘è¶…é•¿æ–‡æœ¬ï¼ˆåˆ†æ®µå¤„ç†ï¼‰

        Args:
            text: è¶…é•¿æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡

        Returns:
            ç¿»è¯‘ç»“æœ
        """
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆä¿ç•™ç©ºè¡Œï¼‰
        paragraphs = text.split('\n\n')

        # åˆ†ç»„ï¼šæ¯ç»„ä¸è¶…è¿‡20000å­—ç¬¦
        groups = []
        current_group = []
        current_length = 0

        for para in paragraphs:
            para_length = len(para)

            if current_length + para_length > 20000 and current_group:
                # å½“å‰ç»„å·²æ»¡ï¼Œå¼€å§‹æ–°ç»„
                groups.append('\n\n'.join(current_group))
                current_group = [para]
                current_length = para_length
            else:
                current_group.append(para)
                current_length += para_length + 2  # +2 for \n\n

        # æ·»åŠ æœ€åä¸€ç»„
        if current_group:
            groups.append('\n\n'.join(current_group))

        print(f"[INFO] Split into {len(groups)} chunks for translation")

        # é€ç»„ç¿»è¯‘
        translations = []
        for i, group in enumerate(groups):
            print(f"[INFO] Translating chunk {i+1}/{len(groups)} ({len(group)} chars)")
            translated = self.translate(group, context)  # é€’å½’è°ƒç”¨ï¼ˆä½†å·²ç»å°äº5ä¸‡äº†ï¼‰
            translations.append(translated)

        return '\n\n'.join(translations)

    def _log_retry_events(self, request_id: int, payload: dict, response: dict, retry_events: list, final_error: Optional[str] = None):
        """
        è®°å½•é‡è¯•äº‹ä»¶åˆ° JSONLï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼‰

        Args:
            request_id: è¯·æ±‚ID
            payload: åŸå§‹è¯·æ±‚ä½“
            response: APIå“åº”ä½“ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
            retry_events: é‡è¯•äº‹ä»¶åˆ—è¡¨ [{"attempt": 1, "error_type": "è¯·æ±‚è¶…æ—¶", ...}, ...]
            final_error: æœ€ç»ˆé”™è¯¯ä¿¡æ¯ï¼ˆæˆåŠŸæ—¶ä¸ºNoneï¼‰
        """
        try:
            self.log_dir.mkdir(parents=True, exist_ok=True)
            log_file = self.log_dir / f"{self.current_file}_retries.jsonl"

            retry_count = len(retry_events)
            total_attempts = retry_count + 1  # æ€»å°è¯•æ¬¡æ•° = é¦–æ¬¡å°è¯• + é‡è¯•æ¬¡æ•°

            # æ„å»ºæ—¥å¿—è®°å½•
            log_entry = {
                "request_id": request_id,
                "timestamp": datetime.now().isoformat(),
                "retry_count": retry_count,  # é‡è¯•æ¬¡æ•°ï¼ˆä¸åŒ…æ‹¬é¦–æ¬¡å°è¯•ï¼‰
                "total_attempts": total_attempts,  # æ€»å°è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬é¦–æ¬¡å°è¯•ï¼‰
                "retry_events": retry_events,
                "final_status": "failed" if final_error else "success",
                "final_error": final_error,
                "request": payload,
                "response": response
            }

            # è¿½åŠ åˆ°é‡è¯•æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        except Exception as e:
            print(f"[WARNING] Failed to log retry events: {e}")

    def _log_translation(self, request_id: int, payload: dict, response: dict,
                        error: Optional[str], attempts: int):
        """
        è®°å½•ç¿»è¯‘è¯·æ±‚å’Œå“åº”åˆ° JSONL æ–‡ä»¶ï¼ˆæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ª .jsonlï¼‰

        Args:
            request_id: è¯·æ±‚ID
            payload: åŸå§‹è¯·æ±‚ä½“
            response: åŸå§‹å“åº”ä½“ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
            error: é”™è¯¯ä¿¡æ¯ï¼ˆæˆåŠŸæ—¶ä¸ºNoneï¼‰
            attempts: å°è¯•æ¬¡æ•°
        """
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            self.log_dir.mkdir(parents=True, exist_ok=True)
            log_file = self.log_dir / f"{self.current_file}.jsonl"

            # æ„å»ºæ—¥å¿—è®°å½•
            log_entry = {
                "request_id": request_id,
                "timestamp": datetime.now().isoformat(),
                "attempts": attempts,
                "request": payload,
                "response": response if response else None,
                "error": error if error else None
            }

            # è¿½åŠ åˆ° JSONL æ–‡ä»¶
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        except Exception as e:
            # æ—¥å¿—å¤±è´¥ä¸å½±å“ç¿»è¯‘ï¼Œä½†æ‰“å°è­¦å‘Š
            print(f"[WARNING] Failed to log translation: {e}")

    def _check_translation_quality(self, original_text: str, translated_text: str) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ç¿»è¯‘è´¨é‡ï¼Œè¯†åˆ«å¼‚å¸¸ç¿»è¯‘ç»“æœ

        Args:
            original_text: åŸæ–‡
            translated_text: è¯‘æ–‡

        Returns:
            (æ˜¯å¦é€šè¿‡æ£€æŸ¥, é—®é¢˜åŸå› )
        """
        if not translated_text or not translated_text.strip():
            return False, "è¯‘æ–‡ä¸ºç©º"

        original_len = len(original_text)
        translated_len = len(translated_text)

        # ===== æ£€æµ‹æç¤ºè¯æ³„æ¼ï¼ˆä¸Šä¸‹æ–‡æ³„æ¼ï¼‰=====
        # æ£€æŸ¥è¯‘æ–‡ä¸­æ˜¯å¦åŒ…å«æç¤ºè¯çš„å…ƒä¿¡æ¯æ ‡è®°
        context_leak_indicators = [
            'ã€å‚è€ƒä¸Šä¸‹æ–‡', 'ã€ä¸è¦ç¿»è¯‘', 'ã€å¾…ç¿»è¯‘',
            'ã€è¯·ç›´æ¥è¾“å‡º', 'ä¸Šæ–‡:', 'ä¸‹æ–‡:',
            'ç« èŠ‚:', 'æ‘˜è¦:', 'å…³é”®è¯:',
            '==============='  # åˆ†éš”ç¬¦æ³„æ¼
        ]
        for indicator in context_leak_indicators:
            if indicator in translated_text:
                return False, f"æç¤ºè¯æ³„æ¼ (åŒ…å«'{indicator}')"

        # ===== è¯†åˆ«ç‰¹æ®Šå†…å®¹ç±»å‹ =====
        # 1. HTMLè¡¨æ ¼
        is_html_table = (
            '<table>' in original_text.lower() or
            '<td>' in original_text.lower() or
            '<tr>' in original_text.lower()
        )

        # 2. ç»“æ„åŒ–æ•°æ®ï¼ˆURLã€é‚®ç®±ã€åˆ—è¡¨ç­‰ï¼‰
        is_structured_data = (
            original_text.count('@') >= 2 or  # å¤šä¸ªé‚®ç®±
            original_text.count('http') >= 2 or  # å¤šä¸ªURL
            original_text.count('$') >= 3  # å¤šä¸ªä»·æ ¼/é‡‘é¢
        )

        # 3. URL/é“¾æ¥ï¼ˆå•ç‹¬çš„URLä¸éœ€è¦ç¿»è¯‘ï¼‰
        is_url_only = (
            (original_text.strip().startswith('http') or
             original_text.strip().startswith('www.') or
             '.com' in original_text or '.org' in original_text) and
            len(original_text.split()) <= 3  # æœ€å¤š3ä¸ªå•è¯
        )

        # 4. è”ç³»ä¿¡æ¯ï¼ˆäººå+é‚®ç®±+ç”µè¯ç­‰ï¼‰
        is_contact_info = (
            '@' in original_text and
            (original_text.count(':') >= 2 or  # E: T: ç­‰æ ‡è®°
             ('+' in original_text and len(original_text) < 200))  # ç”µè¯å·ç 
        )

        # 5. ç‰ˆæƒ/ç½²åä¿¡æ¯
        is_copyright_info = (
            original_text.strip().startswith('Â©') or
            original_text.strip().startswith('BY:') or
            'All rights reserved' in original_text
        )

        # 6. æ£€æµ‹åŸæ–‡æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡ï¼ˆç›®æ ‡è¯­è¨€ï¼‰
        chinese_chars = sum(1 for char in original_text if '\u4e00' <= char <= '\u9fff')
        total_chars = len(original_text.strip())
        is_already_chinese = chinese_chars / max(total_chars, 1) > 0.3  # è¶…è¿‡30%æ˜¯ä¸­æ–‡

        # ç»¼åˆåˆ¤æ–­ï¼šæ˜¯å¦åº”è¯¥è·³è¿‡è´¨é‡æ£€æŸ¥
        should_skip_similarity_check = (
            is_url_only or
            is_contact_info or
            is_copyright_info or
            is_already_chinese
        )

        # ===== æ£€æµ‹å®Œå…¨æœªç¿»è¯‘ï¼ˆåŸæ–‡=è¯‘æ–‡ï¼‰ =====
        # è·³è¿‡ç‰¹å®šç±»å‹å†…å®¹çš„ç›¸ä¼¼åº¦æ£€æŸ¥
        if should_skip_similarity_check:
            pass  # URLã€è”ç³»ä¿¡æ¯ã€ç‰ˆæƒä¿¡æ¯ã€ä¸­æ–‡åŸæ–‡ç­‰ä¸éœ€è¦æ£€æŸ¥ç›¸ä¼¼åº¦
        else:
            # ç§»é™¤ç©ºæ ¼åæ¯”è¾ƒ
            orig_stripped = original_text.replace(' ', '').replace('\n', '')
            trans_stripped = translated_text.replace(' ', '').replace('\n', '')

            # å¦‚æœå»ç©ºæ ¼åè¶…è¿‡90%ç›¸åŒï¼Œè§†ä¸ºæœªç¿»è¯‘
            # ä½†å¯¹HTMLè¡¨æ ¼å’Œç»“æ„åŒ–æ•°æ®æ”¾å®½åˆ°98%ï¼ˆå› ä¸ºæ ‡ç­¾ã€æ•°æ®å¿…é¡»ä¿æŒä¸å˜ï¼‰
            if len(orig_stripped) > 50:  # è‡³å°‘50å­—ç¬¦
                from difflib import SequenceMatcher
                similarity = SequenceMatcher(None, orig_stripped, trans_stripped).ratio()
                similarity_threshold = 0.98 if (is_html_table or is_structured_data) else 0.9
                if similarity > similarity_threshold:
                    return False, f"å®Œå…¨æœªç¿»è¯‘ (ç›¸ä¼¼åº¦{similarity*100:.1f}%)"

        # æ£€æŸ¥è¾“å‡ºé•¿åº¦å¼‚å¸¸ï¼ˆè¯‘æ–‡è¿œè¶…åŸæ–‡ï¼‰
        # æ­£å¸¸ç¿»è¯‘ï¼šè‹±è¯‘ä¸­0.8-1.2å€ï¼Œä¿„è¯‘ä¸­1.0-1.3å€
        # å¯¹äºæçŸ­æ–‡æœ¬ï¼ˆ<20å­—ç¬¦ï¼‰ï¼Œé•¿åº¦æ³¢åŠ¨è¾ƒå¤§ï¼Œæ”¾å®½åˆ°10å€
        # å¯¹äºæ™®é€šæ–‡æœ¬ï¼Œè¯‘æ–‡è¶…è¿‡åŸæ–‡5å€è§†ä¸ºå¼‚å¸¸
        max_ratio = 10 if original_len < 20 else 5
        if translated_len > original_len * max_ratio:
            return False, f"è¯‘æ–‡é•¿åº¦å¼‚å¸¸è¿‡é•¿ (åŸæ–‡{original_len}å­—ç¬¦, è¯‘æ–‡{translated_len}å­—ç¬¦, æ¯”ä¾‹{translated_len/original_len:.1f}å€)"

        # æ£€æŸ¥é‡å¤å†…å®¹ï¼ˆæ¨¡å‹å¹»è§‰å¾ªç¯ï¼‰
        # ===== ä¼˜åŒ–ï¼šHTMLè¡¨æ ¼å’Œç»“æ„åŒ–æ•°æ®è·³è¿‡é‡å¤æ£€æŸ¥ =====
        if is_html_table or is_structured_data:
            pass  # HTMLæ ‡ç­¾ã€åˆ—è¡¨ç»“æ„ã€å¤šä¸ªè”ç³»æ–¹å¼ç­‰æœ¬èº«ä¼šé‡å¤ï¼Œä¸æ˜¯å¹»è§‰
        else:
            # å¦‚æœè¯‘æ–‡ä¸­æœ‰è¿ç»­é‡å¤çš„ç‰‡æ®µï¼ˆé•¿åº¦>20å­—ç¬¦ï¼‰ï¼Œè§†ä¸ºå¼‚å¸¸
            if translated_len > 100:
                # æ£€æµ‹è¿ç»­é‡å¤æ¨¡å¼
                for chunk_size in [20, 30, 50]:
                    for i in range(0, min(200, translated_len - chunk_size)):
                        chunk = translated_text[i:i+chunk_size]
                        # æ£€æŸ¥è¿™ä¸ªç‰‡æ®µæ˜¯å¦åœ¨åç»­é‡å¤å‡ºç°3æ¬¡ä»¥ä¸Š
                        count = translated_text.count(chunk)
                        if count >= 3:
                            return False, f"æ£€æµ‹åˆ°é‡å¤å†…å®¹å¾ªç¯ (ç‰‡æ®µ'{chunk[:10]}...'é‡å¤{count}æ¬¡)"

        # 4. æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹è¾“å‡ºçš„å…ƒä¿¡æ¯ï¼ˆéç¿»è¯‘å†…å®¹ï¼‰
        meta_indicators = [
            "I will translate",
            "Here is the translation",
            "Translation:",
            "The translated text is",
            "I'll help you translate"
        ]
        first_50_chars = translated_text[:50].lower()
        for indicator in meta_indicators:
            if indicator.lower() in first_50_chars:
                return False, f"è¯‘æ–‡åŒ…å«æ¨¡å‹å…ƒä¿¡æ¯ ('{indicator}')"

        # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
        return True, ""

    def _log_quality_issue(self, request_id: int, original_text: str, translated_text: str,
                          issue_reason: str, attempt: int, used_fallback_model: bool = False):
        """
        è®°å½•ç¿»è¯‘è´¨é‡é—®é¢˜åˆ° JSONL

        Args:
            request_id: è¯·æ±‚ID
            original_text: åŸæ–‡
            translated_text: é—®é¢˜è¯‘æ–‡
            issue_reason: é—®é¢˜åŸå› 
            attempt: å°è¯•æ¬¡æ•°
            used_fallback_model: æ˜¯å¦ä½¿ç”¨äº†fallbackæ¨¡å‹
        """
        try:
            self.log_dir.mkdir(parents=True, exist_ok=True)
            log_file = self.log_dir / f"{self.current_file}_quality_issues.jsonl"

            # æ„å»ºæ—¥å¿—è®°å½•
            log_entry = {
                "request_id": request_id,
                "timestamp": datetime.now().isoformat(),
                "attempt": attempt,
                "issue_reason": issue_reason,
                "original_length": len(original_text),
                "translated_length": len(translated_text),
                "length_ratio": len(translated_text) / len(original_text) if len(original_text) > 0 else 0,
                "original_text": original_text[:500],  # åªè®°å½•å‰500å­—ç¬¦
                "translated_text": translated_text[:500],
                "used_model": self.model,  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹
                "used_fallback_model": used_fallback_model  # æ˜¯å¦ä½¿ç”¨äº†fallbackæ¨¡å‹
            }

            # è¿½åŠ åˆ°è´¨é‡é—®é¢˜æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        except Exception as e:
            print(f"[WARNING] Failed to log quality issue: {e}")

    def _log_failed_text(self, text_id: Optional[str], original_text: str, error: str,
                        attempts: int, context: Optional[Dict] = None):
        """
        è®°å½•30æ¬¡é‡è¯•åä»å¤±è´¥çš„æ–‡æœ¬

        Args:
            text_id: æ–‡æœ¬å”¯ä¸€æ ‡è¯†ï¼ˆä¾‹å¦‚ï¼špage_5_item_12ï¼‰
            original_text: åŸæ–‡
            error: é”™è¯¯ä¿¡æ¯
            attempts: å°è¯•æ¬¡æ•°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        try:
            # æ„å»ºæ—¥å¿—è®°å½•
            log_entry = {
                "file_name": self.current_file,
                "text_id": text_id or "unknown",
                "timestamp": datetime.now().isoformat(),
                "attempts": attempts,
                "error": error[:500] if len(error) > 500 else error,  # é™åˆ¶é”™è¯¯é•¿åº¦
                "original_text": original_text[:1000] if len(original_text) > 1000 else original_text,  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                "text_length": len(original_text),
                "context": {
                    "chapter_title": context.get('chapter_title') if context else None,
                    "page_idx": context.get('page_idx') if context else None
                }
            }

            # è¿½åŠ åˆ°æ€»å¤±è´¥æ—¥å¿—
            with open(self.failed_texts_log, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        except Exception as e:
            print(f"[WARNING] Failed to log failed text: {e}")


    def close(self):
        """å…³é—­ Session è¿æ¥æ± """
        if hasattr(self, 'session'):
            self.session.close()

    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­è¿æ¥"""
        self.close()
