"""
ä¸»æµç¨‹è„šæœ¬
å®Œæˆæ•´ä¸ªæ–‡æ¡£ç¿»è¯‘æµç¨‹ï¼š
1. ç”Ÿæˆæ–‡æ¡£å¤§çº²
2. è°ƒç”¨MinerUè§£æPDF
3. æŒ‰é¡µå¤„ç†å†…å®¹å¹¶ç¿»è¯‘
4. ç”ŸæˆHTML
5. è½¬æ¢ä¸ºPDF/DOCX
"""

import yaml
import sys
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from jinja2 import Template

from mineru_client import MinerUClient, FileTask
from mineru_parser import MinerUParser
from article_translator import ArticleTranslator
from logger import Logger
from format_converter import FormatConverter
from outline_generator import OutlineGenerator
from path_manager import PathManager

try:
    from openpyxl import load_workbook
except ImportError:
    load_workbook = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†ä¸»ç±»"""

    def __init__(self, config_path="config.yaml"):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.logger = Logger()
        self.output_base = Path(self.config['paths']['output_base'])

        # åˆå§‹åŒ–MinerUå®¢æˆ·ç«¯
        self.mineru = MinerUClient(
            api_token=self.config['api']['mineru_token'],
            verify_ssl=False,
            max_retries=5
        )

        # åˆå§‹åŒ–è§£æå™¨
        self.parser = MinerUParser()

        # åˆå§‹åŒ–æ ¼å¼è½¬æ¢å™¨
        self.converter = FormatConverter(self.config, self.logger, self.output_base)

        # åˆå§‹åŒ–å¤§çº²ç”Ÿæˆå™¨
        self.outline_gen = OutlineGenerator(self.config, self.logger, self.output_base)

        # åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
        self.path_mgr = PathManager(self.config, self.logger)

        # åˆå§‹åŒ–æ–‡ä»¶å¤¹ç»“æ„
        self._init_directories()

    def _init_directories(self):
        """åˆå§‹åŒ–æ‰€éœ€çš„æ–‡ä»¶å¤¹ç»“æ„"""
        input_base = Path(self.config['paths']['input_base'])
        output_base = Path(self.config['paths']['output_base'])
        terminology_folder = Path(self.config['paths']['terminology_folder'])

        # è¾“å‡ºæ–‡ä»¶å¤¹åç§°
        mineru_folder = self.config['output']['mineru_folder']
        html_folder = self.config['output']['html_folder']
        pdf_folder = self.config['output']['pdf_folder']
        docx_folder = self.config['output']['docx_folder']
        cache_folder = self.config['output']['cache_folder']

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        folders = [
            input_base,
            terminology_folder,
            output_base / mineru_folder,
            output_base / html_folder,
            output_base / pdf_folder,
            output_base / docx_folder,
            output_base / cache_folder / 'outlines',
        ]

        for folder in folders:
            folder.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"æ–‡ä»¶å¤¹ç»“æ„åˆå§‹åŒ–å®Œæˆ")

    def load_terminology_from_excel(self) -> dict:
        """
        ä» terminology æ–‡ä»¶å¤¹ä¸‹çš„ Excel æ–‡ä»¶åŠ è½½æœ¯è¯­åº“

        Returns:
            æœ¯è¯­å­—å…¸ {"English": "ä¸­æ–‡"}
        """
        terminology_folder = Path(self.config['paths']['terminology_folder'])

        if not terminology_folder.exists():
            self.logger.warning(f"æœ¯è¯­åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {terminology_folder}")
            return {}

        if not load_workbook:
            self.logger.warning("openpyxl æœªå®‰è£…ï¼Œæ— æ³•è¯»å– Excel æœ¯è¯­åº“")
            return {}

        glossary = {}
        excel_files = list(terminology_folder.glob("*.xlsx")) + list(terminology_folder.glob("*.xls"))

        if not excel_files:
            self.logger.warning(f"æœ¯è¯­åº“æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰ Excel æ–‡ä»¶: {terminology_folder}")
            return {}

        self.logger.info(f"æ­£åœ¨åŠ è½½æœ¯è¯­åº“ï¼Œå…± {len(excel_files)} ä¸ª Excel æ–‡ä»¶...")

        for excel_file in excel_files:
            try:
                workbook = load_workbook(excel_file, read_only=True, data_only=True)

                # éå†æ‰€æœ‰ sheet
                for sheet_name in workbook.sheetnames:
                    sheet = workbook[sheet_name]

                    # è·³è¿‡ç©º sheet
                    if sheet.max_row <= 1:
                        continue

                    # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯è‹±æ–‡ï¼Œç¬¬äºŒåˆ—æ˜¯ä¸­æ–‡ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
                    for row in sheet.iter_rows(min_row=2, values_only=True):
                        if len(row) >= 2 and row[0] and row[1]:
                            english_term = str(row[0]).strip()
                            chinese_term = str(row[1]).strip()

                            if english_term and chinese_term:
                                glossary[english_term] = chinese_term

                workbook.close()
                self.logger.info(f"  å·²åŠ è½½: {excel_file.name} - {len(glossary)} ä¸ªæœ¯è¯­")

            except Exception as e:
                self.logger.error(f"åŠ è½½ Excel æ–‡ä»¶å¤±è´¥: {excel_file.name} - {str(e)}")

        self.logger.success(f"æœ¯è¯­åº“åŠ è½½å®Œæˆï¼Œå…± {len(glossary)} ä¸ªæœ¯è¯­")
        return glossary

    def batch_process(self):
        """
        æ‰¹é‡å¤„ç† input æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ PDF æ–‡ä»¶ï¼ˆå¤šæ–‡ä»¶å¹¶å‘ï¼‰
        """
        self.logger.info("=" * 60)
        self.logger.info("æ‰¹é‡å¤„ç†æ¨¡å¼")
        self.logger.info("=" * 60)

        # 1. æ‰«æè¾“å…¥æ–‡ä»¶
        file_list = self.path_mgr.scan_input_files()

        if not file_list:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„ PDF æ–‡ä»¶")
            return

        # 2. åŠ è½½å…¨å±€æœ¯è¯­åº“ï¼ˆä» Excelï¼‰
        excel_glossary = self.load_terminology_from_excel()

        # 3. å¤šæ–‡ä»¶å¹¶å‘å¤„ç†
        max_workers = self.config['concurrency']['max_files']
        self.logger.info(f"å¼€å§‹å¹¶å‘å¤„ç†ï¼Œå¹¶å‘æ•°: {max_workers}")

        # ä½¿ç”¨ ProcessPoolExecutor è¿›è¡Œå¤šæ–‡ä»¶å¹¶å‘
        success_count = 0
        failure_count = 0
        results = []

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self._process_single_file, relative_path, pdf_path, excel_glossary):
                (relative_path, pdf_path)
                for relative_path, pdf_path in file_list
            }

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if tqdm:
                future_iterator = tqdm(as_completed(future_to_file), total=len(file_list), desc="å¤„ç†è¿›åº¦")
            else:
                future_iterator = as_completed(future_to_file)

            # æ”¶é›†ç»“æœ
            for future in future_iterator:
                relative_path, pdf_path = future_to_file[future]
                try:
                    result = future.result()
                    if result['success']:
                        success_count += 1
                        self.logger.success(f"âœ“ å®Œæˆ: {relative_path}")
                    else:
                        failure_count += 1
                        self.logger.error(f"âœ— å¤±è´¥: {relative_path} - {result.get('error', 'Unknown error')}")
                    results.append(result)
                except Exception as e:
                    failure_count += 1
                    self.logger.error(f"âœ— å¤±è´¥: {relative_path} - {str(e)}")
                    results.append({'success': False, 'file': relative_path, 'error': str(e)})

        # 4. è¾“å‡ºæ±‡æ€»
        self.logger.info("=" * 60)
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
        self.logger.info(f"  æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"  å¤±è´¥: {failure_count} ä¸ªæ–‡ä»¶")
        self.logger.info("=" * 60)

        return results

    def _process_single_file(self, relative_path: str, pdf_path: str, excel_glossary: dict) -> dict:
        """
        å¤„ç†å•ä¸ª PDF æ–‡ä»¶ï¼ˆç”¨äºå¤šè¿›ç¨‹è°ƒç”¨ï¼‰

        Args:
            relative_path: ç›¸å¯¹è·¯å¾„
            pdf_path: PDF ç»å¯¹è·¯å¾„
            excel_glossary: Excel æœ¯è¯­åº“

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_paths = self.path_mgr.get_output_paths(relative_path)

            # è°ƒç”¨å•æ–‡ä»¶å¤„ç†æµç¨‹
            self.run(pdf_path, output_paths, excel_glossary)

            return {
                'success': True,
                'file': relative_path,
                'output_paths': {k: str(v) for k, v in output_paths.items()}
            }
        except Exception as e:
            return {
                'success': False,
                'file': relative_path,
                'error': str(e)
            }

    def run(self, pdf_path: str, output_paths: dict = None, excel_glossary: dict = None):
        """
        è¿è¡Œå®Œæ•´æµç¨‹

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_paths: è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„å­—å…¸ï¼ˆå¯é€‰ï¼‰
            excel_glossary: Excelæœ¯è¯­åº“ï¼ˆå¯é€‰ï¼‰
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£")
        self.logger.info("=" * 60)

        try:
            # æ­¥éª¤1: ç”Ÿæˆå¤§çº²
            outline = self.outline_gen.generate_outline(pdf_path, output_paths)

            # æ­¥éª¤2: MinerUè§£æ
            content_list = self.parse_with_mineru(pdf_path, output_paths)

            # æ­¥éª¤3: åˆå¹¶æœ¯è¯­åº“ï¼ˆExcel + AIç”Ÿæˆï¼‰
            combined_glossary = {}
            if excel_glossary:
                combined_glossary.update(excel_glossary)
            combined_glossary.update(outline.get('glossary', {}))

            self.logger.info(f"æœ¯è¯­åº“åˆå¹¶å®Œæˆ: {len(combined_glossary)} ä¸ªæœ¯è¯­")

            # æ­¥éª¤4: åˆå§‹åŒ–ç¿»è¯‘å™¨ï¼ˆå¸¦åˆå¹¶åçš„æœ¯è¯­è¡¨ï¼‰
            translator = ArticleTranslator(
                api_key=self.config['api']['translation_api_key'],
                api_url=self.config['api']['translation_api_base_url'],
                model=self.config['api']['translation_api_model'],
                glossary=combined_glossary,
                case_sensitive=False,
                whole_word_only=True,
                config=self.config  # ä¼ é€’configï¼Œç”¨äºè¯»å–APIå‚æ•°å’Œå¹¶å‘é…ç½®
            )

            # æ­¥éª¤5: å¤„ç†å†…å®¹å¹¶ç¿»è¯‘
            original_html, translated_html = self.process_content(
                content_list, outline, translator
            )

            # æ­¥éª¤6: å¯¼å‡ºæ ¼å¼
            self.converter.export_formats(original_html, translated_html, output_paths)

            self.logger.info("=" * 60)
            self.logger.success("å¤„ç†å®Œæˆï¼")
            self.logger.info("=" * 60)

        except Exception as e:
            self.logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise  # æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯exitï¼Œä»¥ä¾¿æ‰¹å¤„ç†èƒ½ç»§ç»­

    def parse_with_mineru(self, pdf_path: str, output_paths: dict = None) -> list:
        """
        ä½¿ç”¨MinerUè§£æPDF

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_paths: è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„å­—å…¸ï¼ˆå¯é€‰ï¼‰

        Returns:
            content_list.jsonå†…å®¹
        """
        self.logger.info("\n>>> æ­¥éª¤2: ä½¿ç”¨MinerUè§£æPDF...")

        # ç¡®å®šç¼“å­˜è·¯å¾„
        if output_paths and 'mineru' in output_paths:
            expected_zip = Path(output_paths['mineru'])
            cache_dir = expected_zip.parent
        else:
            cache_dir = self.output_base / "cache/mineru_results"
            pdf_name = Path(pdf_path).stem
            expected_zip = cache_dir / f"{pdf_name}_result.zip"

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è§£æç»“æœ
        if expected_zip.exists():
            self.logger.info("å‘ç°å·²æœ‰MinerUè§£æç»“æœï¼Œç›´æ¥åŠ è½½...")
            parsed = self.parser.parse_zip_result(
                str(expected_zip),
                source_file_name=Path(pdf_path).name
            )
            self.logger.success(f"è§£æç»“æœå·²åŠ è½½: {len(parsed.json_content)} ä¸ªå†…å®¹å—")
            return parsed.json_content

        # ä¸Šä¼ å¹¶è§£æ
        file_task = FileTask(
            file_name=Path(pdf_path).name,
            file_path=pdf_path,
            data_id=Path(pdf_path).stem
        )

        self.logger.info("æ­£åœ¨ä¸Šä¼ PDFåˆ°MinerU...")
        batch_id, _ = self.mineru.batch_upload_files([file_task])

        self.logger.info("ç­‰å¾…MinerUè§£æå®Œæˆ...")
        results = self.mineru.wait_for_completion(batch_id, poll_interval=10)

        # ä¸‹è½½ç»“æœ
        cache_dir.mkdir(parents=True, exist_ok=True)
        downloaded = self.mineru.download_all_results(results, str(cache_dir))

        # è§£æZIPå¹¶ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
        zip_path = list(downloaded.values())[0]

        # å¦‚æœæœ‰è‡ªå®šä¹‰è·¯å¾„ï¼Œç§»åŠ¨æ–‡ä»¶
        if output_paths and 'mineru' in output_paths:
            import shutil
            shutil.move(zip_path, str(expected_zip))
            zip_path = str(expected_zip)

        parsed = self.parser.parse_zip_result(
            zip_path,
            source_file_name=Path(pdf_path).name
        )

        self.logger.success(f"è§£æå®Œæˆ: {len(parsed.json_content)} ä¸ªå†…å®¹å—")
        return parsed.json_content

    def process_content(
        self,
        content_list: list,
        outline: dict,
        translator: ArticleTranslator
    ) -> tuple:
        """
        å¤„ç†å†…å®¹å¹¶ç¿»è¯‘ï¼ˆä½¿ç”¨æ‰¹é‡å¹¶å‘ç¿»è¯‘ï¼‰

        Args:
            content_list: MinerUè¿”å›çš„content_list
            outline: æ–‡æ¡£å¤§çº²
            translator: ç¿»è¯‘å™¨å®ä¾‹

        Returns:
            (original_html, translated_html) å…ƒç»„
        """
        self.logger.info("\n>>> æ­¥éª¤3: å¤„ç†å†…å®¹å¹¶ç¿»è¯‘...")

        # æŒ‰é¡µåˆ†ç»„
        pages = {}
        for item in content_list:
            page_idx = item.get('page_idx', 0)
            if page_idx not in pages:
                pages[page_idx] = []
            pages[page_idx].append(item)

        self.logger.info(f"å…± {len(pages)} é¡µ")

        # å¤„ç†å›¾ç‰‡ï¼šå¤åˆ¶å›¾ç‰‡åˆ°è¾“å‡ºç›®å½•å¹¶æ›´æ–°è·¯å¾„
        self._process_images(content_list)

        # æ”¶é›†æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
        tasks = []  # [(item, field_name, text, context), ...]
        total_items = sum(len(items) for items in pages.values())

        for page_idx in sorted(pages.keys()):
            items = pages[page_idx]
            context = self._get_chapter_context(page_idx, outline)

            for item in items:
                # è·³è¿‡header/footer/page_number
                if item['type'] in ['header', 'footer', 'page_number']:
                    continue

                # æ”¶é›†æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡
                if item['type'] == 'text' and item.get('text'):
                    tasks.append((item, 'text_zh', item['text'], context))

                # æ”¶é›†å›¾ç‰‡è¯´æ˜ç¿»è¯‘ä»»åŠ¡
                if item['type'] == 'image' and item.get('image_caption'):
                    caption_text = ' '.join(item['image_caption'])
                    tasks.append((item, 'caption_zh', caption_text, context))

        self.logger.info(f"å…±æ”¶é›† {len(tasks)} ä¸ªç¿»è¯‘ä»»åŠ¡ï¼Œå¼€å§‹å¹¶å‘ç¿»è¯‘...")

        # æ‰¹é‡å¹¶å‘ç¿»è¯‘
        translation_tasks = [(text, context) for _, _, text, context in tasks]
        translations = translator.translate_batch(translation_tasks)

        # å°†ç¿»è¯‘ç»“æœèµ‹å€¼å›itemï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œä¸»çº¿ç¨‹æ‰§è¡Œï¼‰
        for i, (item, field_name, _, _) in enumerate(tasks):
            item[field_name] = translations[i]

            # æ¯10%æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % max(1, len(tasks) // 10) == 0:
                progress = (i + 1) * 100 // len(tasks)
                self.logger.info(f"  ç¿»è¯‘è¿›åº¦: {i + 1}/{len(tasks)} ({progress}%)")

        self.logger.success(f"ç¿»è¯‘å®Œæˆ: {len(tasks)} ä¸ªå†…å®¹å—")

        # ç”ŸæˆHTML
        self.logger.info("æ­£åœ¨ç”ŸæˆHTML...")
        original_html = self._render_html(pages, language='en')
        translated_html = self._render_html(pages, language='zh')

        # ä¿å­˜HTML
        html_dir = self.output_base / "html"
        html_dir.mkdir(parents=True, exist_ok=True)

        (html_dir / "original.html").write_text(original_html, encoding='utf-8')
        (html_dir / "translated.html").write_text(translated_html, encoding='utf-8')

        self.logger.success(f"HTMLå·²ç”Ÿæˆ: {html_dir}")

        return original_html, translated_html

    def _process_images(self, content_list: list):
        """
        å¤„ç†å›¾ç‰‡ï¼šå¤åˆ¶å›¾ç‰‡åˆ°HTMLè¾“å‡ºç›®å½•å¹¶æ›´æ–°è·¯å¾„

        Args:
            content_list: å†…å®¹åˆ—è¡¨
        """
        import shutil

        # ç¡®å®šMinerUè§£å‹ç›®å½•
        mineru_folder = self.config['output']['mineru_folder']
        mineru_dir = self.output_base / mineru_folder

        if not mineru_dir.exists():
            self.logger.warning(f"æœªæ‰¾åˆ°MinerUè¾“å‡ºç›®å½•: {mineru_dir}ï¼Œè·³è¿‡å›¾ç‰‡å¤„ç†")
            return

        # æŸ¥æ‰¾æœ€æ–°çš„è§£å‹ç›®å½•ï¼ˆåŒ…å«imageså­ç›®å½•çš„ç›®å½•ï¼‰
        extract_dirs = []
        for item in mineru_dir.rglob("*"):
            if item.is_dir() and (item / "images").exists():
                extract_dirs.append(item)

        if not extract_dirs:
            self.logger.warning("æœªæ‰¾åˆ°åŒ…å«imagesç›®å½•çš„MinerUè§£å‹ç»“æœ")
            return

        # ä½¿ç”¨æœ€æ–°çš„ç›®å½•
        latest_dir = max(extract_dirs, key=lambda d: d.stat().st_mtime)
        source_images_dir = latest_dir / "images"

        self.logger.info(f"æ‰¾åˆ°MinerUå›¾ç‰‡ç›®å½•: {source_images_dir}")

        # åˆ›å»ºç›®æ ‡å›¾ç‰‡ç›®å½•ï¼ˆHTML/imagesï¼‰
        html_folder = self.config['output']['html_folder']
        html_dir = self.output_base / html_folder
        target_images_dir = html_dir / "images"
        target_images_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶å›¾ç‰‡å¹¶æ›´æ–°è·¯å¾„
        copied_count = 0
        for item in content_list:
            if item.get('type') == 'image' and item.get('img_path'):
                img_rel_path = item['img_path']  # ä¾‹å¦‚: "images/xxx.jpg"

                # æ„å»ºæºæ–‡ä»¶è·¯å¾„
                source_img = latest_dir / img_rel_path

                if source_img.exists():
                    # æå–æ–‡ä»¶å
                    img_filename = Path(img_rel_path).name

                    # å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•
                    target_img = target_images_dir / img_filename
                    shutil.copy2(source_img, target_img)

                    # æ›´æ–°itemä¸­çš„è·¯å¾„ï¼ˆç›¸å¯¹äºHTMLæ–‡ä»¶ï¼‰
                    item['img_path'] = f"images/{img_filename}"
                    copied_count += 1

        if copied_count > 0:
            self.logger.success(f"å·²å¤åˆ¶ {copied_count} å¼ å›¾ç‰‡åˆ° {target_images_dir}")
        else:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶")

    def _get_chapter_context(self, page_idx: int, outline: dict) -> dict:
        """
        è·å–é¡µé¢å¯¹åº”çš„ç« èŠ‚ä¸Šä¸‹æ–‡

        Args:
            page_idx: é¡µé¢ç´¢å¼•
            outline: æ–‡æ¡£å¤§çº²

        Returns:
            ç« èŠ‚ä¸Šä¸‹æ–‡å­—å…¸
        """
        for chapter in outline.get('structure', []):
            pages = chapter.get('pages', [])
            if len(pages) >= 2:
                start, end = pages[0], pages[1]
                if start <= page_idx <= end:
                    return {
                        'chapter_title': chapter.get('title', ''),
                        'chapter_summary': chapter.get('summary', ''),
                        'keywords': chapter.get('keywords', [])
                    }
        return {}

    def _render_html(self, pages: dict, language: str) -> str:
        """
        æ¸²æŸ“HTML

        Args:
            pages: æŒ‰é¡µåˆ†ç»„çš„å†…å®¹
            language: è¯­è¨€ï¼ˆ'en'æˆ–'zh'ï¼‰

        Returns:
            HTMLå­—ç¬¦ä¸²
        """
        with open('page_template.html', 'r', encoding='utf-8') as f:
            template = Template(f.read())

        return template.render(pages=pages, language=language)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼
    if len(sys.argv) == 1:
        interactive_mode()
        return

    # æ‰¹å¤„ç†æ¨¡å¼
    if sys.argv[1] in ["--batch", "-b", "--interactive", "-i"]:
        interactive_mode()
    else:
        # å¦‚æœæä¾›äº†å‚æ•°ä½†ä¸æ˜¯å·²çŸ¥é€‰é¡¹ï¼Œæ˜¾ç¤ºé”™è¯¯
        print(f"âŒ æœªçŸ¥å‚æ•°: {sys.argv[1]}")
        print("ä½¿ç”¨ 'python main.py -h' æŸ¥çœ‹å¸®åŠ©")
        sys.exit(1)

def interactive_mode():
    """äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢"""
    processor = DocumentProcessor()

    while True:
        print("\n" + "="*60)
        print("  MinerU æ–‡æ¡£ç¿»è¯‘å·¥å…· - äº¤äº’æ¨¡å¼")
        print("="*60)
        print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
        print("  [1] æ‰¹é‡å¤„ç†ï¼ˆé€’å½’æ‰«æ input/ æ–‡ä»¶å¤¹ï¼‰")
        print("  [2] æŸ¥çœ‹é…ç½®ä¿¡æ¯")
        print("  [3] æŸ¥çœ‹è¾“å…¥æ–‡ä»¶åˆ—è¡¨")
        print("  [4] æ¸…é™¤ç¼“å­˜")
        print("  [0] é€€å‡º")
        print()

        choice = input("è¯·è¾“å…¥é€‰é¡¹ [0-4]: ").strip()

        if choice == "0":
            print("\nå†è§ï¼")
            break
        elif choice == "1":
            batch_mode_interactive(processor)
        elif choice == "2":
            show_config(processor)
        elif choice == "3":
            show_input_files(processor)
        elif choice == "4":
            clear_cache(processor)
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")


def batch_mode_interactive(processor):
    """æ‰¹é‡å¤„ç†äº¤äº’æ¨¡å¼"""
    print("\n" + "-"*60)
    print("  æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("-"*60)

    # æ‰«ææ–‡ä»¶
    file_list = processor.path_mgr.scan_input_files()

    if not file_list:
        print("\nâŒ input/ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        print("   è¯·å…ˆå°† PDF æ–‡ä»¶æ”¾å…¥ input/ æ–‡ä»¶å¤¹")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    print(f"\næ‰¾åˆ° {len(file_list)} ä¸ª PDF æ–‡ä»¶:")
    for i, (rel_path, abs_path) in enumerate(file_list[:10], 1):
        print(f"  {i}. {rel_path}")

    if len(file_list) > 10:
        print(f"  ... è¿˜æœ‰ {len(file_list) - 10} ä¸ªæ–‡ä»¶")

    print(f"\nå¹¶å‘é…ç½®:")
    print(f"  - æ–‡ä»¶å¹¶å‘æ•°: {processor.config['concurrency']['max_files']}")
    print(f"  - ç¿»è¯‘å¹¶å‘æ•°: {processor.config['concurrency']['initial_translation_workers']} (åˆå§‹)")

    confirm = input(f"\nç¡®è®¤å¼€å§‹æ‰¹é‡å¤„ç†ï¼Ÿ[y/N]: ").strip().lower()

    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    try:
        print("\nå¼€å§‹æ‰¹é‡å¤„ç†...")
        processor.batch_process()
        print("\nâœ“ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def show_config(processor):
    """æ˜¾ç¤ºé…ç½®ä¿¡æ¯"""
    print("\n" + "-"*60)
    print("  å½“å‰é…ç½®ä¿¡æ¯")
    print("-"*60)

    config = processor.config

    print("\nğŸ“¡ API é…ç½®:")
    print(f"  MinerU Token: {'å·²é…ç½®' if config['api']['mineru_token'] != 'YOUR_MINERU_TOKEN' else 'âŒ æœªé…ç½®'}")
    print(f"  Outline API Key: {'å·²é…ç½®' if config['api']['outline_api_key'] != 'YOUR_GEMINI_KEY' else 'âŒ æœªé…ç½®'}")
    print(f"  Outline API URL: {config['api']['outline_api_base_url']}")
    print(f"  Outline API Model: {config['api']['outline_api_model']}")
    print(f"  Translation API Key: {'å·²é…ç½®' if config['api']['translation_api_key'] else 'âŒ æœªé…ç½®'}")
    print(f"  Translation API URL: {config['api']['translation_api_base_url']}")
    print(f"  Translation API Model: {config['api']['translation_api_model']}")

    print("\nâš™ï¸ API å‚æ•°:")
    print(f"  Temperature: {config['api']['temperature']}")
    print(f"  Max Tokens: {config['api']['max_tokens']}")
    print(f"  Timeout: {config['api']['timeout']}s")

    print("\nğŸ”„ å¹¶å‘é…ç½®:")
    print(f"  æ–‡ä»¶å¹¶å‘æ•°: {config['concurrency']['max_files']}")
    print(f"  åˆå§‹ç¿»è¯‘å¹¶å‘: {config['concurrency']['initial_translation_workers']}")
    print(f"  æœ€å¤§ç¿»è¯‘å¹¶å‘: {config['concurrency']['max_translation_workers']}")
    print(f"  æœ€å°ç¿»è¯‘å¹¶å‘: {config['concurrency']['min_translation_workers']}")

    print("\nğŸ“‚ è·¯å¾„é…ç½®:")
    print(f"  è¾“å…¥ç›®å½•: {config['paths']['input_base']}")
    print(f"  è¾“å‡ºç›®å½•: {config['paths']['output_base']}")
    print(f"  æœ¯è¯­åº“ç›®å½•: {config['paths']['terminology_folder']}")

    print("\nğŸ“„ è¾“å‡ºæ ¼å¼:")
    print(f"  æ ¼å¼: {', '.join(config['output']['formats'])}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def show_input_files(processor):
    """æ˜¾ç¤ºè¾“å…¥æ–‡ä»¶åˆ—è¡¨"""
    print("\n" + "-"*60)
    print("  è¾“å…¥æ–‡ä»¶åˆ—è¡¨")
    print("-"*60)

    file_list = processor.path_mgr.scan_input_files()

    if not file_list:
        print("\nâŒ input/ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        print("   è¯·å…ˆå°† PDF æ–‡ä»¶æ”¾å…¥ input/ æ–‡ä»¶å¤¹")
    else:
        print(f"\næ‰¾åˆ° {len(file_list)} ä¸ª PDF æ–‡ä»¶:\n")
        for i, (rel_path, abs_path) in enumerate(file_list, 1):
            file_size = Path(abs_path).stat().st_size / (1024 * 1024)  # MB
            print(f"  {i:3d}. {rel_path:50s} ({file_size:.1f} MB)")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def clear_cache(processor):
    """æ¸…é™¤ç¼“å­˜"""
    print("\n" + "-"*60)
    print("  æ¸…é™¤ç¼“å­˜")
    print("-"*60)

    cache_dir = processor.output_base / "cache"

    if not cache_dir.exists():
        print("\næ²¡æœ‰ç¼“å­˜éœ€è¦æ¸…é™¤")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    print("\nç¼“å­˜ç›®å½•:")
    print(f"  {cache_dir}")

    # ç»Ÿè®¡ç¼“å­˜å¤§å°
    total_size = 0
    file_count = 0
    for file in cache_dir.rglob("*"):
        if file.is_file():
            total_size += file.stat().st_size
            file_count += 1

    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    print(f"  æ–‡ä»¶æ•°: {file_count}")
    print(f"  æ€»å¤§å°: {total_size / (1024 * 1024):.1f} MB")

    confirm = input("\nç¡®è®¤æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼Ÿ[y/N]: ").strip().lower()

    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    try:
        import shutil
        shutil.rmtree(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)
        print("\nâœ“ ç¼“å­˜å·²æ¸…é™¤")
    except Exception as e:
        print(f"\nâŒ æ¸…é™¤å¤±è´¥: {str(e)}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == "__main__":
    main()
