"""
ä¸»æµç¨‹è„šæœ¬ - é‡æ„ç‰ˆ

æ ¸å¿ƒèŒè´£ï¼šæµç¨‹ç¼–æ’å™¨ (Orchestrator)
- åè°ƒå„ä¸ªæ¨¡å—å®Œæˆæ–‡æ¡£ç¿»è¯‘å·¥ä½œæµ
- è´Ÿè´£é«˜å±‚æµç¨‹æ§åˆ¶ï¼Œä¸åŒ…å«å…·ä½“ä¸šåŠ¡é€»è¾‘å®ç°

é‡æ„è®°å½•ï¼ˆ2025-12-15ï¼‰ï¼š
1. æå– MinerU æ‰¹å¤„ç†é€»è¾‘ â†’ mineru_batch_processor.py
2. æå–ç¿»è¯‘ä»»åŠ¡ç®¡ç†é€»è¾‘ â†’ translation_task_manager.py
3. æå–å†…å®¹å¤„ç†è¾…åŠ©å‡½æ•° â†’ content_helpers.py
4. ä»£ç é‡ï¼š1822è¡Œ â†’ 1110è¡Œï¼ˆå‡å°‘39%ï¼‰
5. å¹³å‡æ–¹æ³•é•¿åº¦ï¼š76è¡Œ â†’ çº¦35è¡Œï¼ˆå‡å°‘54%ï¼‰
"""

import yaml
import os
import sys
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor, wait, FIRST_COMPLETED
from jinja2 import Template
import shutil
import threading
import queue
import time
import hashlib

from mineru_client import MinerUClient, FileTask, TaskState
from mineru_parser import MinerUParser
from article_translator import ArticleTranslator
from logger import Logger
from format_converter import FormatConverter
from outline_generator import OutlineGenerator
from path_manager import PathManager
from resume_manager import ResumeManager
from mineru_batch_processor import MinerUBatchProcessor
from translation_task_manager import TranslationTaskManager
from content_helpers import (
    process_images, merge_split_texts,
    group_narrow_images, get_chapter_context
)

try:
    from openpyxl import load_workbook
except ImportError:
    load_workbook = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†ä¸»ç±»"""

    def __init__(self, config_path="config.yaml"):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.logger = Logger()
        self.output_base = Path(self.config['paths']['output_base'])

        # åˆå§‹åŒ–MinerUå®¢æˆ·ç«¯
        self.mineru = MinerUClient(
            api_token=self.config['api']['mineru_token'],
            model_version="pipeline",
            verify_ssl=False,
            max_retries=self.config['retry']['mineru_max_retries']
        )

        # åˆå§‹åŒ–è§£æå™¨ï¼ˆä¿®æ”¹è¾“å‡ºç›®å½•åˆ°output/MinerUï¼‰
        mineru_output_dir = self.output_base / self.config['output']['mineru_folder']
        self.parser = MinerUParser(output_dir=str(mineru_output_dir))

        # åˆå§‹åŒ–æ ¼å¼è½¬æ¢å™¨
        self.converter = FormatConverter(self.config, self.logger, self.output_base)

        # åˆå§‹åŒ–å¤§çº²ç”Ÿæˆå™¨
        self.outline_gen = OutlineGenerator(self.config, self.logger, self.output_base)

        # åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
        self.path_mgr = PathManager(self.config, self.logger)

        # åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
        self.resume_mgr = ResumeManager(self.logger)

        # åˆå§‹åŒ–æ–‡ä»¶å¤¹ç»“æ„
        self._init_directories()

    def _init_directories(self):
        """åˆå§‹åŒ–æ‰€éœ€çš„æ–‡ä»¶å¤¹ç»“æ„"""
        input_base = Path(self.config['paths']['input_base'])
        output_base = Path(self.config['paths']['output_base'])
        terminology_folder = Path(self.config['paths']['terminology_folder'])

        # è¾“å‡ºæ–‡ä»¶å¤¹åç§°
        mineru_folder = self.config['output']['mineru_folder']
        html_folder = self.config['output']['html_folder']
        pdf_folder = self.config['output']['pdf_folder']
        docx_folder = self.config['output']['docx_folder']
        cache_folder = self.config['output']['cache_folder']

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        folders = [
            input_base,
            terminology_folder,
            output_base / mineru_folder,
            output_base / html_folder,
            output_base / pdf_folder,
            output_base / docx_folder,
            output_base / cache_folder / 'outlines',
        ]

        for folder in folders:
            folder.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"æ–‡ä»¶å¤¹ç»“æ„åˆå§‹åŒ–å®Œæˆ")

        # æ¸…ç†æ®‹ç•™çš„ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_files()

    def _cleanup_temp_files(self):
        """æ¸…ç†æ®‹ç•™çš„ä¸´æ—¶åˆ†å‰²å’Œåˆå¹¶æ–‡ä»¶"""
        input_base = Path(self.config['paths']['input_base'])
        output_base = Path(self.config['paths']['output_base'])
        mineru_folder = self.config['output']['mineru_folder']

        cleanup_count = 0

        # 1. æ¸…ç† input ç›®å½•ä¸‹çš„ temp_splits
        for temp_dir in input_base.rglob("temp_splits"):
            if temp_dir.is_dir():
                try:
                    import shutil
                    shutil.rmtree(temp_dir)
                    cleanup_count += 1
                except Exception as e:
                    self.logger.warning(f"æ— æ³•æ¸…ç† {temp_dir}: {e}")

        # 2. æ¸…ç† output/MinerU ä¸‹çš„ temp_parts
        mineru_base = output_base / mineru_folder
        if mineru_base.exists():
            for temp_dir in mineru_base.rglob("temp_parts"):
                if temp_dir.is_dir():
                    try:
                        import shutil
                        shutil.rmtree(temp_dir)
                        cleanup_count += 1
                    except Exception as e:
                        self.logger.warning(f"æ— æ³•æ¸…ç† {temp_dir}: {e}")

        # 3. æ¸…ç† input ç›®å½•ä¸‹çš„ _compressed.pdf æ–‡ä»¶ï¼ˆæ—§å‹ç¼©æ–‡ä»¶ï¼‰
        for compressed_file in input_base.rglob("*_compressed.pdf"):
            try:
                compressed_file.unlink()
                cleanup_count += 1
            except Exception as e:
                self.logger.warning(f"æ— æ³•åˆ é™¤ {compressed_file}: {e}")

        if cleanup_count > 0:
            self.logger.info(f"å·²æ¸…ç† {cleanup_count} ä¸ªä¸´æ—¶æ–‡ä»¶/ç›®å½•")

    def load_terminology_from_excel(self) -> dict:
        """
        ä» terminology æ–‡ä»¶å¤¹ä¸‹çš„ Excel æ–‡ä»¶åŠ è½½æœ¯è¯­åº“

        Returns:
            æœ¯è¯­å­—å…¸ {"English": "ä¸­æ–‡"}
        """
        terminology_folder = Path(self.config['paths']['terminology_folder'])

        if not terminology_folder.exists():
            self.logger.warning(f"æœ¯è¯­åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {terminology_folder}")
            return {}

        if not load_workbook:
            self.logger.warning("openpyxl æœªå®‰è£…ï¼Œæ— æ³•è¯»å– Excel æœ¯è¯­åº“")
            return {}

        glossary = {}
        excel_files = list(terminology_folder.glob("*.xlsx")) + list(terminology_folder.glob("*.xls"))

        if not excel_files:
            self.logger.warning(f"æœ¯è¯­åº“æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰ Excel æ–‡ä»¶: {terminology_folder}")
            return {}

        self.logger.info(f"æ­£åœ¨åŠ è½½æœ¯è¯­åº“ï¼Œå…± {len(excel_files)} ä¸ª Excel æ–‡ä»¶...")

        for excel_file in excel_files:
            try:
                workbook = load_workbook(excel_file, read_only=True, data_only=True)

                for sheet_name in workbook.sheetnames:
                    sheet = workbook[sheet_name]

                    if sheet.max_row <= 1:
                        continue

                    for row in sheet.iter_rows(min_row=2, values_only=True):
                        if len(row) >= 2 and row[0] and row[1]:
                            english_term = str(row[0]).strip()
                            chinese_term = str(row[1]).strip()

                            if english_term and chinese_term:
                                glossary[english_term] = chinese_term

                workbook.close()
                self.logger.info(f"  å·²åŠ è½½: {excel_file.name} - {len(glossary)} ä¸ªæœ¯è¯­")

            except Exception as e:
                self.logger.error(f"åŠ è½½ Excel æ–‡ä»¶å¤±è´¥: {excel_file.name} - {str(e)}")

        self.logger.success(f"æœ¯è¯­åº“åŠ è½½å®Œæˆï¼Œå…± {len(glossary)} ä¸ªæœ¯è¯­")
        return glossary

    def batch_process(self):
        """
        æ‰¹é‡å¤„ç† input æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ PDF æ–‡ä»¶
        æµæ°´çº¿æ¨¡å¼ï¼šMinerU è§£æå®Œä¸€ä¸ªæ–‡ä»¶ç«‹å³å¼€å§‹ç¿»è¯‘ï¼Œä¸ç­‰å¾…æ•´æ‰¹å®Œæˆ
        """
        self.logger.info("=" * 60)
        self.logger.info("æ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆæµæ°´çº¿ä¼˜åŒ– - è§£æç¿»è¯‘å¹¶è¡Œï¼‰")
        self.logger.info("=" * 60)

        # 1. æ‰«æè¾“å…¥æ–‡ä»¶
        file_list = self.path_mgr.scan_input_files()

        if not file_list:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„ PDF æ–‡ä»¶")
            return

        # 2. åŠ è½½å…¨å±€æœ¯è¯­åº“ï¼ˆä» Excelï¼‰
        excel_glossary = self.load_terminology_from_excel()

        # 3. ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æ£€æŸ¥æ–‡ä»¶çŠ¶æ€
        categorized = self.resume_mgr.categorize_files(file_list, self.path_mgr)

        # å¦‚æœå…¨éƒ¨å·²å®Œæˆï¼Œç›´æ¥è¿”å›
        if self.resume_mgr.is_all_completed(categorized):
            self.logger.success("æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
            return []

        # å‡†å¤‡å¤„ç†åˆ—è¡¨
        files_to_upload, ready_to_translate = self.resume_mgr.prepare_processing_lists(categorized)
        already_completed = [status.relative_path for status in categorized['completed']]

        # 4. åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœæ”¶é›†
        translation_queue = queue.Queue()  # MinerUå®Œæˆçš„æ–‡ä»¶æ”¾å…¥æ­¤é˜Ÿåˆ—
        failed_files = []  # è®°å½•MinerUå¤±è´¥çš„æ–‡ä»¶
        failed_files_lock = threading.Lock()
        results = []
        results_lock = threading.Lock()

        # å…ˆæŠŠå·²æœ‰ç»“æœçš„æ–‡ä»¶åŠ å…¥é˜Ÿåˆ—
        for item in ready_to_translate:
            translation_queue.put(item)

        # 5. å¯åŠ¨ç¿»è¯‘å·¥ä½œçº¿ç¨‹æ± 
        max_workers = self.config['concurrency']['max_files']
        self.logger.info(f"\n>>> å¯åŠ¨ç¿»è¯‘å·¥ä½œæ±  (å¹¶å‘æ•°: {max_workers})...")

        stop_event = threading.Event()
        translation_futures = []

        executor = ProcessPoolExecutor(max_workers=max_workers)

        # 6. å¦‚æœæœ‰éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå¯åŠ¨ MinerU ç›‘æ§çº¿ç¨‹
        monitor_thread = None
        if files_to_upload:
            self.logger.info(f"\n>>> å¯åŠ¨ MinerU ä¸Šä¼ å’Œç›‘æ§...")
            # åˆ›å»º MinerU æ‰¹å¤„ç†å™¨
            mineru_processor = MinerUBatchProcessor(
                self.mineru, self.logger, self.config, self.path_mgr
            )
            monitor_thread = threading.Thread(
                target=mineru_processor.upload_and_monitor,
                args=(files_to_upload, translation_queue, stop_event, failed_files, failed_files_lock),
                daemon=True
            )
            monitor_thread.start()
        else:
            # æ²¡æœ‰éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶ï¼Œç›´æ¥è®¾ç½®åœæ­¢äº‹ä»¶
            stop_event.set()

        # 7. ç¿»è¯‘ä»»åŠ¡æäº¤çº¿ç¨‹ï¼šä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶æäº¤åˆ°è¿›ç¨‹æ± 
        # åªå¤„ç†éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼ˆæ’é™¤å·²å®Œæˆçš„ï¼‰
        total_files = len(ready_to_translate) + len(files_to_upload)

        if total_files == 0:
            self.logger.success("æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
            return []

        self.logger.info(f"\n>>> å¯åŠ¨ç¿»è¯‘ä»»åŠ¡è°ƒåº¦ (å…± {total_files} ä¸ªæ–‡ä»¶ï¼Œå·²è·³è¿‡ {len(already_completed)} ä¸ª)...")

        future_to_file = {}  # {future: relative_path}
        future_lock = threading.Lock()
        submitted_count = 0

        def submit_tasks():
            """ä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶æäº¤åˆ°è¿›ç¨‹æ± """
            nonlocal submitted_count
            while submitted_count < total_files:
                try:
                    # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                    relative_path, pdf_path, mineru_zip_path = translation_queue.get(timeout=5)

                    self.logger.info(f"[æäº¤] {relative_path}")

                    # æäº¤ç¿»è¯‘ä»»åŠ¡
                    future = executor.submit(
                        self._process_translation_only,
                        relative_path,
                        pdf_path,
                        excel_glossary,
                        mineru_zip_path
                    )

                    with future_lock:
                        future_to_file[future] = relative_path
                        submitted_count += 1

                except queue.Empty:
                    # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©º
                    if stop_event.is_set() and translation_queue.empty():
                        # MinerUå·²å®Œæˆä¸”é˜Ÿåˆ—ä¸ºç©º
                        break
                    continue

        # å¯åŠ¨æäº¤çº¿ç¨‹
        submit_thread = threading.Thread(target=submit_tasks, daemon=True)
        submit_thread.start()

        # 8. æ”¶é›†ç¿»è¯‘ç»“æœ
        self.logger.info(f"\n>>> å¼€å§‹æ”¶é›†ç¿»è¯‘ç»“æœ...")
        success_count = 0
        failure_count = 0
        processed_count = 0

        if tqdm:
            pbar = tqdm(total=total_files, desc="æ€»è¿›åº¦")

        # ä½¿ç”¨ as_completed æ”¶é›†ç»“æœ
        while processed_count < total_files:
            # ç­‰å¾…è‡³å°‘æœ‰ä¸€ä¸ªä»»åŠ¡æäº¤
            should_wait = False
            with future_lock:
                if not future_to_file:
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¤±è´¥çš„æ–‡ä»¶æ²¡ç»Ÿè®¡
                    with failed_files_lock:
                        failed_count = len(failed_files)

                    # å¦‚æœå·²å¤„ç†æ•° + å¤±è´¥æ•° = æ€»æ•°ï¼Œè¯´æ˜å…¨éƒ¨å®Œæˆ
                    if processed_count + failed_count >= total_files:
                        break

                    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ä½†å·²ç»å…¨éƒ¨æäº¤å®Œï¼Œé€€å‡º
                    if submitted_count + failed_count >= total_files:
                        break
                    # å¦åˆ™éœ€è¦ç­‰å¾…
                    should_wait = True

            if should_wait:
                time.sleep(0.5)
                continue

            # æ”¶é›†å·²å®Œæˆçš„ä»»åŠ¡
            done_futures = []
            with future_lock:
                for future in list(future_to_file.keys()):
                    if future.done():
                        done_futures.append(future)

            for future in done_futures:
                with future_lock:
                    relative_path = future_to_file.pop(future)

                try:
                    result = future.result()
                    if result['success']:
                        success_count += 1
                        self.logger.success(f"âœ“ å®Œæˆ: {relative_path}")
                    else:
                        failure_count += 1
                        self.logger.error(f"âœ— å¤±è´¥: {relative_path} - {result.get('error', 'Unknown')}")
                    results.append(result)

                except Exception as e:
                    failure_count += 1
                    self.logger.error(f"âœ— å¼‚å¸¸: {relative_path} - {str(e)}")
                    results.append({'success': False, 'file': relative_path, 'error': str(e)})

                processed_count += 1
                if tqdm:
                    pbar.update(1)

            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
            if processed_count >= total_files:
                break

            # çŸ­æš‚ä¼‘çœ 
            if not done_futures:
                time.sleep(0.1)

        if tqdm:
            pbar.close()

        # 9. ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        submit_thread.join(timeout=10)
        if monitor_thread:
            monitor_thread.join(timeout=10)
        executor.shutdown(wait=True)

        # 10. è¾“å‡ºæ±‡æ€»ï¼ˆåŒ…å«è·³è¿‡çš„æ–‡ä»¶å’Œå¤±è´¥çš„æ–‡ä»¶ï¼‰
        # å°†å·²å®Œæˆçš„æ–‡ä»¶åŠ å…¥ç»“æœ
        for relative_path in already_completed:
            results.append({
                'success': True,
                'file': relative_path,
                'skipped': True,
                'reason': 'å·²å­˜åœ¨å®Œæ•´è¾“å‡º'
            })

        # å°†MinerUå¤±è´¥çš„æ–‡ä»¶åŠ å…¥ç»“æœ
        with failed_files_lock:
            for relative_path, error_msg in failed_files:
                results.append({
                    'success': False,
                    'file': relative_path,
                    'error': error_msg
                })
                failure_count += 1

        total_count = len(results) + len(already_completed)
        self.logger.info("=" * 60)
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
        self.logger.info(f"  æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"  å¤±è´¥: {failure_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"  è·³è¿‡: {len(already_completed)} ä¸ªæ–‡ä»¶ï¼ˆå·²å®Œæˆï¼‰")
        self.logger.info(f"  æ€»è®¡: {len(file_list)} ä¸ªæ–‡ä»¶")
        self.logger.info("=" * 60)
        self.logger.info("ç¿»è¯‘è¯·æ±‚æ—¥å¿—å·²ä¿å­˜åˆ° logs/translation/ ç›®å½•")

        return results

    def _batch_upload_to_mineru(self, file_list: list) -> dict:
        """
        æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ° MinerUï¼ˆæ¯æ‰¹æœ€å¤š200ä¸ªæ–‡ä»¶ï¼‰

        Args:
            file_list: [(relative_path, pdf_path), ...] æ–‡ä»¶åˆ—è¡¨

        Returns:
            {relative_path: zip_path} å­—å…¸ï¼Œæ˜ å°„ç›¸å¯¹è·¯å¾„åˆ°ä¸‹è½½çš„ZIPæ–‡ä»¶è·¯å¾„
        """
        # 1. æ£€æŸ¥å“ªäº›æ–‡ä»¶å·²ç»æœ‰ç»“æœï¼Œå“ªäº›éœ€è¦ä¸Šä¼ 
        files_to_upload = []
        existing_results = {}

        mineru_folder = self.config['output']['mineru_folder']
        mineru_dir = self.output_base / mineru_folder

        for relative_path, pdf_path in file_list:
            output_paths = self.path_mgr.get_output_paths(relative_path)
            expected_zip = Path(output_paths['mineru'])

            if expected_zip.exists():
                self.logger.info(f"âœ“ å·²å­˜åœ¨: {relative_path}")
                existing_results[relative_path] = str(expected_zip)
            else:
                files_to_upload.append((relative_path, pdf_path, output_paths))

        if not files_to_upload:
            self.logger.success("æ‰€æœ‰æ–‡ä»¶éƒ½å·²æœ‰ MinerU è§£æç»“æœï¼Œè·³è¿‡ä¸Šä¼ ")
            return existing_results

        self.logger.info(f"éœ€è¦ä¸Šä¼ : {len(files_to_upload)} ä¸ªæ–‡ä»¶")
        self.logger.info(f"å·²æœ‰ç»“æœ: {len(existing_results)} ä¸ªæ–‡ä»¶")

        # 2. åˆ†æ‰¹ä¸Šä¼ ï¼ˆæ¯æ‰¹200ä¸ªï¼‰
        BATCH_SIZE = 200
        all_results = existing_results.copy()

        for batch_idx in range(0, len(files_to_upload), BATCH_SIZE):
            batch_files = files_to_upload[batch_idx:batch_idx + BATCH_SIZE]
            batch_num = batch_idx // BATCH_SIZE + 1
            total_batches = (len(files_to_upload) + BATCH_SIZE - 1) // BATCH_SIZE

            self.logger.info(f"\n--- æ‰¹æ¬¡ {batch_num}/{total_batches} (å…± {len(batch_files)} ä¸ªæ–‡ä»¶) ---")

            # åˆ›å»º FileTask åˆ—è¡¨
            file_tasks = []
            for relative_path, pdf_path, output_paths in batch_files:
                file_task = FileTask(
                    file_name=Path(pdf_path).name,
                    file_path=pdf_path,
                    data_id=Path(pdf_path).stem
                )
                file_tasks.append(file_task)

            try:
                # æ‰¹é‡ä¸Šä¼ 
                self.logger.info(f"æ­£åœ¨ä¸Šä¼ æ‰¹æ¬¡ {batch_num}...")
                batch_id, _, split_info = self.mineru.batch_upload_files(file_tasks)

                # ç­‰å¾…å®Œæˆ
                self.logger.info(f"ç­‰å¾…æ‰¹æ¬¡ {batch_num} è§£æå®Œæˆ...")
                results = self.mineru.wait_for_completion(batch_id, poll_interval=10)

                # ä¸‹è½½ç»“æœ
                self.logger.info(f"ä¸‹è½½æ‰¹æ¬¡ {batch_num} ç»“æœ...")

                for i, (relative_path, pdf_path, output_paths) in enumerate(batch_files):
                    result = results[i]

                    if result.state == TaskState.DONE and result.full_zip_url:
                        # ä¸‹è½½åˆ°æŒ‡å®šä½ç½®
                        expected_zip = Path(output_paths['mineru'])
                        expected_zip.parent.mkdir(parents=True, exist_ok=True)

                        # ç”Ÿæˆä¿å­˜æ–‡ä»¶å
                        base_name = Path(pdf_path).stem
                        zip_name = f"{base_name}_result.zip"

                        try:
                            zip_path = self.mineru.download_result(
                                result.full_zip_url,
                                str(expected_zip.parent),
                                zip_name
                            )

                            # å¦‚æœä¸‹è½½ä½ç½®ä¸æ˜¯ç›®æ ‡ä½ç½®ï¼Œç§»åŠ¨æ–‡ä»¶
                            if Path(zip_path) != expected_zip:
                                shutil.move(zip_path, str(expected_zip))
                                zip_path = str(expected_zip)

                            all_results[relative_path] = zip_path
                            self.logger.success(f"âœ“ {relative_path}")

                        except Exception as e:
                            self.logger.error(f"âœ— ä¸‹è½½å¤±è´¥: {relative_path} - {str(e)}")
                    else:
                        error_msg = result.err_msg or f"çŠ¶æ€: {result.state.value}"
                        self.logger.error(f"âœ— è§£æå¤±è´¥: {relative_path} - {error_msg}")

            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {str(e)}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
                continue

        self.logger.success(f"\nMinerU æ‰¹é‡ä¸Šä¼ å®Œæˆï¼æˆåŠŸ: {len(all_results)}/{len(file_list)} ä¸ªæ–‡ä»¶")
        return all_results

    def _process_translation_only(
        self,
        relative_path: str,
        pdf_path: str,
        excel_glossary: dict,
        mineru_zip_path: str
    ) -> dict:
        """
        åªå¤„ç†ç¿»è¯‘å’Œæ ¼å¼è½¬æ¢ï¼ˆMinerU è§£æå·²å®Œæˆï¼‰

        Args:
            relative_path: ç›¸å¯¹è·¯å¾„
            pdf_path: PDF ç»å¯¹è·¯å¾„
            excel_glossary: Excel æœ¯è¯­åº“
            mineru_zip_path: MinerU ç»“æœ ZIP æ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            if not mineru_zip_path or not Path(mineru_zip_path).exists():
                return {
                    'success': False,
                    'file': relative_path,
                    'error': 'MinerU è§£æç»“æœä¸å­˜åœ¨'
                }

            output_paths = self.path_mgr.get_output_paths(relative_path)

            # æ™ºèƒ½æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥ HTML æ˜¯å¦å·²å­˜åœ¨
            html_original_path = Path(output_paths['html_original'])
            html_translated_path = Path(output_paths['html_translated'])

            if html_translated_path.exists() and html_original_path.exists():
                # HTML å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
                self.logger.info(f"æ£€æµ‹åˆ°å·²æœ‰ HTMLï¼Œè·³è¿‡ç¿»è¯‘ï¼Œåªè¡¥å…¨æ ¼å¼è½¬æ¢: {relative_path}")
                original_html = html_original_path.read_text(encoding='utf-8')
                translated_html = html_translated_path.read_text(encoding='utf-8')
            else:
                # HTML ä¸å­˜åœ¨ï¼Œéœ€è¦å®Œæ•´å¤„ç†
                # 1. è§£æ MinerU ç»“æœ
                parsed = self.parser.parse_zip_result(
                    mineru_zip_path,
                    source_file_name=Path(pdf_path).name
                )

                extract_dir = self.parser.output_dir / Path(mineru_zip_path).stem

                # 2. ç”Ÿæˆå¤§çº²
                outline = self.outline_gen.generate_outline(pdf_path, output_paths)

                # 3. åˆå§‹åŒ–ç¿»è¯‘å™¨
                translator = ArticleTranslator(
                    api_key=self.config['api']['translation_api_key'],
                    api_url=self.config['api']['translation_api_base_url'],
                    model=self.config['api']['translation_api_model'],
                    glossary=excel_glossary or {},
                    case_sensitive=False,
                    whole_word_only=True,
                    config=self.config
                )

                # è®¾ç½®å½“å‰æ–‡ä»¶åï¼ˆç”¨äºæ—¥å¿—ï¼‰
                translator.current_file = Path(relative_path).stem

                # 4. å¤„ç†å†…å®¹å¹¶ç¿»è¯‘
                original_html, translated_html = self.process_content(
                    parsed.json_content,
                    outline,
                    translator,
                    str(extract_dir),
                    output_paths
                )

            # 5. å¯¼å‡ºæ ¼å¼ï¼ˆä¼šæ™ºèƒ½è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
            self.converter.export_formats(original_html, translated_html, output_paths)

            return {
                'success': True,
                'file': relative_path,
                'output_paths': {k: str(v) for k, v in output_paths.items()}
            }

        except Exception as e:
            return {
                'success': False,
                'file': relative_path,
                'error': str(e)
            }

    def _process_single_file(self, relative_path: str, pdf_path: str, excel_glossary: dict) -> dict:
        """
        å¤„ç†å•ä¸ª PDF æ–‡ä»¶ï¼ˆç”¨äºå¤šè¿›ç¨‹è°ƒç”¨ï¼‰

        Args:
            relative_path: ç›¸å¯¹è·¯å¾„
            pdf_path: PDF ç»å¯¹è·¯å¾„
            excel_glossary: Excel æœ¯è¯­åº“

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            output_paths = self.path_mgr.get_output_paths(relative_path)
            self.run(pdf_path, output_paths, excel_glossary)

            return {
                'success': True,
                'file': relative_path,
                'output_paths': {k: str(v) for k, v in output_paths.items()}
            }
        except Exception as e:
            return {
                'success': False,
                'file': relative_path,
                'error': str(e)
            }

    def run(self, pdf_path: str, output_paths: dict = None, excel_glossary: dict = None):
        """
        è¿è¡Œå®Œæ•´æµç¨‹

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_paths: è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„å­—å…¸ï¼ˆå¯é€‰ï¼‰
            excel_glossary: Excelæœ¯è¯­åº“ï¼ˆå¯é€‰ï¼‰
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£")
        self.logger.info("=" * 60)

        try:
            # æ­¥éª¤1: ç”Ÿæˆå¤§çº²
            outline = self.outline_gen.generate_outline(pdf_path, output_paths)

            # æ­¥éª¤2: MinerUè§£æ
            content_list, extract_dir = self.parse_with_mineru(pdf_path, output_paths)

            # æ­¥éª¤3: ä½¿ç”¨ Excel æœ¯è¯­åº“ï¼ˆä¸ä½¿ç”¨ AI ç”Ÿæˆçš„æœ¯è¯­ï¼‰
            combined_glossary = excel_glossary or {}
            
            if combined_glossary:
                self.logger.info(f"æœ¯è¯­åº“åŠ è½½å®Œæˆ: {len(combined_glossary)} ä¸ªæœ¯è¯­")
            else:
                self.logger.warning("æœªæ‰¾åˆ°æœ¯è¯­åº“ï¼Œå°†ä¸è¿›è¡Œæœ¯è¯­é¢„æ›¿æ¢")

            # æ­¥éª¤4: åˆå§‹åŒ–ç¿»è¯‘å™¨
            translator = ArticleTranslator(
                api_key=self.config['api']['translation_api_key'],
                api_url=self.config['api']['translation_api_base_url'],
                model=self.config['api']['translation_api_model'],
                glossary=combined_glossary,
                case_sensitive=False,
                whole_word_only=True,
                config=self.config
            )

            # æ­¥éª¤5: å¤„ç†å†…å®¹å¹¶ç¿»è¯‘
            original_html, translated_html = self.process_content(
                content_list, outline, translator, extract_dir, output_paths
            )

            # æ­¥éª¤6: å¯¼å‡ºæ ¼å¼
            self.converter.export_formats(original_html, translated_html, output_paths)

            self.logger.info("=" * 60)
            self.logger.success("å¤„ç†å®Œæˆï¼")
            self.logger.info("=" * 60)

        except Exception as e:
            self.logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    def parse_with_mineru(self, pdf_path: str, output_paths: dict = None) -> tuple:
        """
        ä½¿ç”¨MinerUè§£æPDF

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_paths: è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„å­—å…¸ï¼ˆå¯é€‰ï¼‰

        Returns:
            (content_list, extract_dir) - å†…å®¹åˆ—è¡¨å’Œè§£å‹ç›®å½•
        """
        self.logger.info("\n>>> æ­¥éª¤2: ä½¿ç”¨MinerUè§£æPDF...")

        # ç¡®å®šZIPä¿å­˜è·¯å¾„ï¼ˆoutput/MinerU/ç›¸å¯¹è·¯å¾„ï¼‰
        if output_paths and 'mineru' in output_paths:
            expected_zip = Path(output_paths['mineru'])
        else:
            mineru_folder = self.config['output']['mineru_folder']
            mineru_dir = self.output_base / mineru_folder
            pdf_name = Path(pdf_path).stem
            expected_zip = mineru_dir / f"{pdf_name}_result.zip"

        expected_zip.parent.mkdir(parents=True, exist_ok=True)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è§£æç»“æœ
        if expected_zip.exists():
            self.logger.info("å‘ç°å·²æœ‰MinerUè§£æç»“æœï¼Œç›´æ¥åŠ è½½...")
            parsed = self.parser.parse_zip_result(
                str(expected_zip),
                source_file_name=Path(pdf_path).name
            )
            # è·å–è§£å‹ç›®å½•
            extract_dir = self.parser.output_dir / Path(expected_zip).stem
            self.logger.success(f"è§£æç»“æœå·²åŠ è½½: {len(parsed.json_content)} ä¸ªå†…å®¹å—")
            return parsed.json_content, str(extract_dir)

        # ä¸Šä¼ å¹¶è§£æ
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„MD5å“ˆå¸Œå‰16ä½ä½œä¸º data_idï¼ˆä¿è¯ä¸è¶…è¿‡128å­—ç¬¦ï¼‰
        data_id = hashlib.md5(pdf_path.encode('utf-8')).hexdigest()[:16]

        file_task = FileTask(
            file_name=Path(pdf_path).name,
            file_path=pdf_path,
            data_id=data_id
        )

        self.logger.info("æ­£åœ¨ä¸Šä¼ PDFåˆ°MinerU...")
        batch_id, _, split_info = self.mineru.batch_upload_files([file_task])

        self.logger.info("ç­‰å¾…MinerUè§£æå®Œæˆ...")
        results = self.mineru.wait_for_completion(batch_id, poll_interval=10)

        # å¦‚æœæ–‡ä»¶è¢«åˆ†å‰²äº†ï¼Œéœ€è¦åˆå¹¶ç»“æœ
        if pdf_path in split_info:
            self.logger.info(f"æ–‡ä»¶è¢«åˆ†å‰²ä¸º {len(split_info[pdf_path])} éƒ¨åˆ†ï¼Œå‡†å¤‡ä¸‹è½½å¹¶åˆå¹¶...")

            # ä¸‹è½½æ‰€æœ‰éƒ¨åˆ†
            part_zips = []
            page_offsets = []

            for part_idx, (task_idx, start_page, end_page) in enumerate(split_info[pdf_path], 1):
                result = results[task_idx]

                if result.state == TaskState.DONE and result.full_zip_url:
                    # ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
                    temp_dir = expected_zip.parent / "temp_parts"
                    temp_dir.mkdir(parents=True, exist_ok=True)

                    part_name = f"{Path(pdf_path).stem}_part{part_idx}_result.zip"
                    zip_path = self.mineru.download_result(
                        result.full_zip_url,
                        str(temp_dir),
                        part_name
                    )

                    part_zips.append(zip_path)
                    page_offsets.append(start_page)
                    self.logger.success(f"âœ“ Part {part_idx} ä¸‹è½½å®Œæˆ")
                else:
                    error_msg = result.err_msg or "æœªçŸ¥é”™è¯¯"
                    raise RuntimeError(f"Part {part_idx} è§£æå¤±è´¥: {error_msg}")

            # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
            self.logger.info("æ­£åœ¨åˆå¹¶æ‰€æœ‰éƒ¨åˆ†...")
            self.mineru._merge_mineru_results(part_zips, str(expected_zip), page_offsets)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for part_zip in part_zips:
                try:
                    Path(part_zip).unlink()
                except:
                    pass

            self.logger.success("âœ“ æ‰€æœ‰éƒ¨åˆ†å·²åˆå¹¶")

        else:
            # æœªåˆ†å‰²ï¼šç›´æ¥ä¸‹è½½
            downloaded = self.mineru.download_all_results(results, str(expected_zip.parent))

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸä¸‹è½½
            if not downloaded:
                error_msg = "MinerUè§£æå¤±è´¥ï¼Œæ²¡æœ‰å¯ä¸‹è½½çš„ç»“æœã€‚"
                # æ£€æŸ¥resultsä¸­çš„å¤±è´¥åŸå› 
                for result in results:
                    if result.state == TaskState.FAILED:
                        reason = result.err_msg or 'æœªçŸ¥åŸå› '
                        error_msg += f"\nå¤±è´¥åŸå› : {reason}"
                        error_msg += "\n\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:"
                        error_msg += "\n1. æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦æŸåæˆ–åŠ å¯†"
                        error_msg += "\n2. å°è¯•é‡æ–°ä¸‹è½½æˆ–è½¬æ¢PDFæ–‡ä»¶"
                        error_msg += "\n3. æ£€æŸ¥PDFæ–‡ä»¶å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶"
                self.logger.error(error_msg)
                raise RuntimeError(error_msg)

            # è·å–ä¸‹è½½çš„zipæ–‡ä»¶è·¯å¾„
            zip_path = list(downloaded.values())[0]

            # å¦‚æœä¸‹è½½ä½ç½®ä¸æ˜¯ç›®æ ‡ä½ç½®ï¼Œç§»åŠ¨æ–‡ä»¶
            if Path(zip_path) != expected_zip:
                shutil.move(zip_path, str(expected_zip))

        # è§£æZIP
        parsed = self.parser.parse_zip_result(
            str(expected_zip),
            source_file_name=Path(pdf_path).name
        )

        # è·å–è§£å‹ç›®å½•
        extract_dir = self.parser.output_dir / Path(expected_zip).stem

        self.logger.success(f"è§£æå®Œæˆ: {len(parsed.json_content)} ä¸ªå†…å®¹å—")
        return parsed.json_content, str(extract_dir)

    def process_content(
        self,
        content_list: list,
        outline: dict,
        translator: ArticleTranslator,
        extract_dir: str,
        output_paths: dict = None
    ) -> tuple:
        """
        å¤„ç†å†…å®¹å¹¶ç¿»è¯‘

        Args:
            content_list: MinerUè¿”å›çš„content_list
            outline: æ–‡æ¡£å¤§çº²
            translator: ç¿»è¯‘å™¨å®ä¾‹
            extract_dir: MinerUè§£å‹ç›®å½•
            output_paths: è¾“å‡ºè·¯å¾„å­—å…¸

        Returns:
            (original_html, translated_html) å…ƒç»„
        """
        self.logger.info("\n>>> æ­¥éª¤3: å¤„ç†å†…å®¹å¹¶ç¿»è¯‘...")

        # æŒ‰é¡µåˆ†ç»„
        pages = {}
        for item in content_list:
            page_idx = item.get('page_idx', 0)
            if page_idx not in pages:
                pages[page_idx] = []
            pages[page_idx].append(item)

        self.logger.info(f"å…± {len(pages)} é¡µ")

        # å¤„ç†å›¾ç‰‡ï¼šå¤åˆ¶åˆ°HTMLç›®å½•å¹¶æ›´æ–°è·¯å¾„
        process_images(content_list, extract_dir, output_paths, self.logger, self.config)

        # æç®€åˆå¹¶ï¼šå¤„ç†è¿å­—ç¬¦æ–­è¯å’Œè·¨åˆ—åˆ†å‰²
        for page_idx in pages.keys():
            items = pages[page_idx]
            merged_items = merge_split_texts(items)
            pages[page_idx] = merged_items

        # åˆ›å»ºç¿»è¯‘ä»»åŠ¡ç®¡ç†å™¨
        task_mgr = TranslationTaskManager(self.logger, self.config)

        # åŠ è½½å¤±è´¥æ–‡æœ¬ç¼“å­˜
        failed_texts_cache = task_mgr.load_failed_cache()

        # æ”¶é›†ç¿»è¯‘ä»»åŠ¡
        tasks = task_mgr.collect_tasks(pages, outline, get_chapter_context)

        # æ‰§è¡Œç¿»è¯‘
        translations = task_mgr.execute_translations(tasks, translator)

        # åˆ†é…ç¿»è¯‘ç»“æœ
        retry_stats = task_mgr.assign_results(tasks, translations, failed_texts_cache)

        # æ›´æ–°å¤±è´¥æ—¥å¿—
        task_mgr.update_failed_log(failed_texts_cache, retry_stats)

        # ç”ŸæˆHTML
        self.logger.info("æ­£åœ¨ç”ŸæˆHTML...")

        # å¯¹å›¾ç‰‡è¿›è¡Œæ™ºèƒ½åˆ†ç»„ï¼ˆè¿ç»­çš„çª„é•¿å›¾ç‰‡åˆå¹¶æˆä¸€è¡Œï¼‰
        pages = group_narrow_images(pages, self.logger)

        original_html = self._render_html(pages, language='en')
        translated_html = self._render_html(pages, language='zh')

        self.logger.success("HTMLå·²ç”Ÿæˆ")

        return original_html, translated_html

    def _render_html(self, pages: dict, language: str) -> str:
        """æ¸²æŸ“HTML"""
        with open('page_template.html', 'r', encoding='utf-8') as f:
            template = Template(f.read())

        return template.render(pages=pages, language=language)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    if len(sys.argv) == 1:
        interactive_mode()
        return

    if sys.argv[1] in ["--batch", "-b", "--interactive", "-i"]:
        interactive_mode()
    else:
        print(f"âŒ æœªçŸ¥å‚æ•°: {sys.argv[1]}")
        print("ä½¿ç”¨ 'python main.py -h' æŸ¥çœ‹å¸®åŠ©")
        sys.exit(1)

def interactive_mode():
    """äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢"""
    processor = DocumentProcessor()

    while True:
        print("\n" + "="*60)
        print("  MinerU æ–‡æ¡£ç¿»è¯‘å·¥å…· - äº¤äº’æ¨¡å¼")
        print("="*60)
        print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
        print("  [1] æ‰¹é‡å¤„ç†ï¼ˆé€’å½’æ‰«æ input/ æ–‡ä»¶å¤¹ï¼‰")
        print("  [2] æŸ¥çœ‹é…ç½®ä¿¡æ¯")
        print("  [3] æŸ¥çœ‹è¾“å…¥æ–‡ä»¶åˆ—è¡¨")
        print("  [4] æ¸…é™¤ç¼“å­˜")
        print("  [0] é€€å‡º")
        print()

        choice = input("è¯·è¾“å…¥é€‰é¡¹ [0-4]: ").strip()

        if choice == "0":
            print("\nå†è§ï¼")
            break
        elif choice == "1":
            batch_mode_interactive(processor)
        elif choice == "2":
            show_config(processor)
        elif choice == "3":
            show_input_files(processor)
        elif choice == "4":
            clear_cache(processor)
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")


def batch_mode_interactive(processor):
    """æ‰¹é‡å¤„ç†äº¤äº’æ¨¡å¼"""
    print("\n" + "-"*60)
    print("  æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("-"*60)

    file_list = processor.path_mgr.scan_input_files()

    if not file_list:
        print("\nâŒ input/ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        print("   è¯·å…ˆå°† PDF æ–‡ä»¶æ”¾å…¥ input/ æ–‡ä»¶å¤¹")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    print(f"\næ‰¾åˆ° {len(file_list)} ä¸ª PDF æ–‡ä»¶:")
    for i, (rel_path, abs_path) in enumerate(file_list[:10], 1):
        print(f"  {i}. {rel_path}")

    if len(file_list) > 10:
        print(f"  ... è¿˜æœ‰ {len(file_list) - 10} ä¸ªæ–‡ä»¶")

    print(f"\nå¹¶å‘é…ç½®:")
    print(f"  - æ–‡ä»¶å¹¶å‘æ•°: {processor.config['concurrency']['max_files']}")
    print(f"  - ç¿»è¯‘å¹¶å‘æ•°: {processor.config['concurrency']['initial_translation_workers']} (åˆå§‹)")

    confirm = input(f"\nç¡®è®¤å¼€å§‹æ‰¹é‡å¤„ç†ï¼Ÿ[y/N]: ").strip().lower()

    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    try:
        print("\nå¼€å§‹æ‰¹é‡å¤„ç†...")
        processor.batch_process()
        print("\nâœ“ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def show_config(processor):
    """æ˜¾ç¤ºé…ç½®ä¿¡æ¯"""
    print("\n" + "-"*60)
    print("  å½“å‰é…ç½®ä¿¡æ¯")
    print("-"*60)

    config = processor.config

    print("\nğŸ“¡ API é…ç½®:")
    print(f"  MinerU Token: {'å·²é…ç½®' if config['api']['mineru_token'] != 'YOUR_MINERU_TOKEN' else 'âŒ æœªé…ç½®'}")
    print(f"  Translation API: {config['api']['translation_api_base_url']}")
    print(f"  Translation Model: {config['api']['translation_api_model']}")

    print("\nğŸ”„ å¹¶å‘é…ç½®:")
    print(f"  æ–‡ä»¶å¹¶å‘æ•°: {config['concurrency']['max_files']}")
    print(f"  ç¿»è¯‘å¹¶å‘æ•°: {config['concurrency']['initial_translation_workers']}-{config['concurrency']['max_translation_workers']}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def show_input_files(processor):
    """æ˜¾ç¤ºè¾“å…¥æ–‡ä»¶åˆ—è¡¨"""
    print("\n" + "-"*60)
    print("  è¾“å…¥æ–‡ä»¶åˆ—è¡¨")
    print("-"*60)

    file_list = processor.path_mgr.scan_input_files()

    if not file_list:
        print("\nâŒ input/ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
    else:
        print(f"\næ‰¾åˆ° {len(file_list)} ä¸ª PDF æ–‡ä»¶:\n")
        for i, (rel_path, abs_path) in enumerate(file_list, 1):
            file_size = Path(abs_path).stat().st_size / (1024 * 1024)
            print(f"  {i:3d}. {rel_path:50s} ({file_size:.1f} MB)")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


def clear_cache(processor):
    """æ¸…é™¤ç¼“å­˜"""
    print("\n" + "-"*60)
    print("  æ¸…é™¤ç¼“å­˜")
    print("-"*60)

    cache_dir = processor.output_base / "cache"

    if not cache_dir.exists():
        print("\næ²¡æœ‰ç¼“å­˜éœ€è¦æ¸…é™¤")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    total_size = 0
    file_count = 0
    for file in cache_dir.rglob("*"):
        if file.is_file():
            total_size += file.stat().st_size
            file_count += 1

    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    print(f"  æ–‡ä»¶æ•°: {file_count}")
    print(f"  æ€»å¤§å°: {total_size / (1024 * 1024):.1f} MB")

    confirm = input("\nç¡®è®¤æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼Ÿ[y/N]: ").strip().lower()

    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        return

    try:
        shutil.rmtree(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)
        print("\nâœ“ ç¼“å­˜å·²æ¸…é™¤")
    except Exception as e:
        print(f"\nâŒ æ¸…é™¤å¤±è´¥: {str(e)}")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == "__main__":
    main()